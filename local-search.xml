<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>18.Photo OCR 应用实例:图片文字识别</title>
    <link href="/2025/02/16/18%20Photo%20OCR%20%E5%BA%94%E7%94%A8%E5%AE%9E%E4%BE%8B%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB/"/>
    <url>/2025/02/16/18%20Photo%20OCR%20%E5%BA%94%E7%94%A8%E5%AE%9E%E4%BE%8B%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h1 id="18-Photo-OCR-应用实例-图片文字识别"><a href="#18-Photo-OCR-应用实例-图片文字识别" class="headerlink" title="18 Photo OCR 应用实例:图片文字识别"></a>18 Photo OCR 应用实例:图片文字识别</h1><h2 id="18-1-问题描述和流程图"><a href="#18-1-问题描述和流程图" class="headerlink" title="18.1 问题描述和流程图"></a>18.1 问题描述和流程图</h2><p>图像文字识别需要如下步骤:</p><p>1.文字侦测(Text detection)——将图片上的文字与其他环境对象分离开来<br>2.字符切分(Character segmentation)——将文字分割成一个个单一的字符<br>3.字符分类(Character classification)——确定每一个字符是什么</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219220605275.png" alt="image-20250219220605275" style="zoom:33%;" /><p>如果用任务流程图来表达这个问题，每一项任务可以由一个单独的小队来负责：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219220835347.png" alt="image-20250219220835347" style="zoom:33%;" /><h2 id="18-2-滑动窗口"><a href="#18-2-滑动窗口" class="headerlink" title="18.2 滑动窗口"></a>18.2 滑动窗口</h2><p>滑动窗口是一项用来从图像中抽取对象的技术。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219221134419.png" alt="image-20250219221134419" style="zoom:33%;" /><p>假使需要在图片中识别行人，首先用许多固定尺寸的图片 82x36，来训练一个能够准确识别行人的模型。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219222533511.png" alt="image-20250219222533511" style="zoom:33%;" /><p>之后使用训练模型时用的图片尺寸对预测图片进行剪裁，将切片交给模型判断其是否为行人，然后滑动剪裁区域，剪裁切片后再交给模型判断，直至将图片全部检测完。</p><p>然后按比例放大剪裁的区域，以新的尺寸对图片进行剪裁，将新剪裁的切片按比例缩小至模型采纳的尺寸 82x36 ，交给模型判断，如此循环。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219222718919.png" alt="image-20250219222718919" style="zoom: 50%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219222743025.png" alt="image-20250219222743025" style="zoom:50%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219222754263.png" alt="image-20250219222754263" style="zoom:50%;" /><p>滑动窗口技术也被用于文字识别，首先训练模型使其能够区分字符与非字符，然后，运用滑动窗口技术识别字符。一旦完成了识别，将识别得出的区域进行一些扩展（膨胀），然后将重叠的区域进行合并。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219222837705.png" alt="image-20250219222837705" style="zoom: 50%;" /><p>接着以宽高比作为过滤条件，过滤高度比宽度更大的区域(因为单词的长度通常比高度要大)。下图中红框是识别出的文字区域，其余白色区域是被忽略的。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219223036852.png" alt="image-20250219223036852" style="zoom:33%;" /><p>下一步是训练模型分割字符，需要的训练集为单个字符的图和两个相连字符之间的图。模型训练完后，仍然使用滑动窗口技术来进行字符分割。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219223119388.png" alt="image-20250219223119388" style="zoom: 50%;" /><p>最后是字符分类阶段，利用神经网络、支持向量机或者逻辑回归算法训练一个分类器即可。</p><h2 id="18-3-获取大量数据和人工数据"><a href="#18-3-获取大量数据和人工数据" class="headerlink" title="18.3 获取大量数据和人工数据"></a>18.3 获取大量数据和人工数据</h2><p>以文字识别应用为例，一种方法是从网站下载各种字体，利用不同的字体配上各种不同的随机背景，创造出一些用于训练的实例，能够获得一个无限大的训练集。这属于从零开始创造实例。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219224905635.png" alt="image-20250219224905635" style="zoom:50%;" /><p>另一种方法是，对已有的数据进行修改，例如将其进行扭曲、旋转、模糊处理。只要认为实际数据有可能和处理后的数据类似，便可以用这样的方法来创造大量的数据。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219224938326.png" alt="image-20250219224938326" style="zoom:50%;" /><p> 某些处理起不到增加数据集的作用，例如加入高斯噪声、将单个图片复制多份等。要考虑实际数据中是否有和处理后的数据类似。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219225004524.png" alt="image-20250219225004524" style="zoom:50%;" /><p>在增大数据集之前，1. 有用吗？画出学习曲线，必须确保当前训练的已经是一个低偏差、高方差的模型。不然增大数据集也没有用。2. 花多少时间？ </p><p>有关获得更多数据的几种方法: 1.人工数据合成；2.手动收集、标记数据；3.众包</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219225129872.png" alt="image-20250219225129872" style="zoom: 50%;" /><h2 id="18-4-上限分析-What-Part-of-the-Pipeline-to-Work-on-Next"><a href="#18-4-上限分析-What-Part-of-the-Pipeline-to-Work-on-Next" class="headerlink" title="18.4 上限分析 What Part of the Pipeline to Work on Next"></a>18.4 上限分析 What Part of the Pipeline to Work on Next</h2><p>如何知道哪一部分最值得花时间和精力去改善呢?可以使用流程图进行上限分析，流程图中每一部分的输出都是下一部分的输入。</p><p>在上限分析中，我们选取一部分，手工提供 100%正确的输出结果，然后看应用的整体效果提升了多少。假使当前总体效果为 72%的正确率：<br>1)如果让文字检测部分100%正确，系统的总体效果从 72%提高到了89%。这意味着很值得投入时间精力来提高我们的文字检测的准确度<br>2)接着让字符切分结果100%正确，系统总体效果只提升了 1%，这意味着字符切分部分可能已经足够好了<br>3)最后让字符分类100%正确，系统总体效果又提升了10%，这意味着我们可能也会应该投入更多的时间和精力到分类这部分</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219225944218.png" alt="image-20250219225944218" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219230128961.png" alt="image-20250219230128961" style="zoom:33%;" /><p>其中最值得优化的是：脸部检测Face detection，眼部分割Eyes segmentation 和 逻辑回归Logistic regression 三个部分。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219230419856.png" alt="image-20250219230419856" style="zoom:33%;" /><p>建议不要根据直觉，而是使用上限分析判断应该改进哪个模块。当把精力花在最值得优化的那个模块上，会让整个系统的表现有显著的提高。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219231254838.png" alt="image-20250219231254838" style="zoom:33%;" />]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>17.大规模机器学习</title>
    <link href="/2025/02/16/17%20%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <url>/2025/02/16/17%20%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="17-大规模机器学习"><a href="#17-大规模机器学习" class="headerlink" title="17 大规模机器学习"></a>17 大规模机器学习</h1><h2 id="17-1-大型数据集的学习"><a href="#17-1-大型数据集的学习" class="headerlink" title="17.1 大型数据集的学习"></a>17.1 大型数据集的学习</h2><p>如果有一个低方差的模型， 通常通过增加数据集的规模，可以获得更好的结果。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219200544140.png" alt="image-20250219200544140" style="zoom:33%;" /><p>但是如果数据集特别大，则首先应该检查这么大规模是否真的必要，也许只用 1000个训练集也能获得较好的效果，可以绘制学习曲线来帮助判断。判断是高偏差还是高方差，高偏差时增加数据集是没有用的，这时考虑设计更多特征；高方差时，增加数据集通常能让算法表现更好。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219201037808.png" alt="image-20250219201037808" style="zoom:33%;" /><h2 id="17-2-随机梯度下降法-Stochastic-Gradient-Descent"><a href="#17-2-随机梯度下降法-Stochastic-Gradient-Descent" class="headerlink" title="17.2 随机梯度下降法 Stochastic Gradient Descent"></a>17.2 随机梯度下降法 Stochastic Gradient Descent</h2><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219201516874.png" alt="image-20250219201516874" style="zoom:33%;" /><p>batch gradient desent 每次迭代需要使用全量训练集，直到算法收敛</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219201757150.png" alt="image-20250219201757150" style="zoom:33%;" /><p>如果必须使用一个大规模的训练集，则可以尝试使用随机梯度下降法(SGD)来代替批量梯度下降法(batch gradient desent)。</p><p>批量梯度下降法每次迭代需要使用全量训练集，直到算法收敛。随机梯度下降算法 则首先对训练集随机“打乱”，然后在每一次计算(只计算一个样本)之后便更新参数$\theta$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219202353537.png" alt="image-20250219202353537" style="zoom:33%;" /><p>在批量梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但 SGD 不是每一步都是朝着”正确”的方向迈出的。因此虽然会逐渐走向全局最小值的位置，但可能无法到达最小值点，而是在附近徘徊。不过很多时候这已经足够了。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219202815649.png" alt="image-20250219202815649" style="zoom:33%;" /><h2 id="17-3-小批量梯度下降-Mini-Batch-Gradient-Descent"><a href="#17-3-小批量梯度下降-Mini-Batch-Gradient-Descent" class="headerlink" title="17.3 小批量梯度下降 Mini-Batch Gradient Descent"></a>17.3 小批量梯度下降 Mini-Batch Gradient Descent</h2><p>小批量梯度下降算法，介于批量梯度下降算法和随机梯度下降算法之间，每计算b个训练样本，更新一次参数 $\theta$ 。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219203802057.png" alt="image-20250219203802057" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219204118115.png" alt="image-20250219204118115" style="zoom:33%;" /><p>通常会令 b 在 2-100 之间。小批量梯度下降的好处在于可以用向量化的方式来循环b个训练实例，如果用的线性代数函数库能支持并行处理，那算法的总体表现将与随机梯度下降更快。</p><h2 id="17-4-随机梯度下降算法的收敛"><a href="#17-4-随机梯度下降算法的收敛" class="headerlink" title="17.4 随机梯度下降算法的收敛"></a>17.4 随机梯度下降算法的收敛</h2><p>在批量梯度下降中，可以令代价函数 J 为迭代次数的函数，绘制图表判断梯度下降是否收敛。但是，在大规模的训练集下不现实，因为计算代价太大。</p><p>当数据集很大时使用随机梯度下降算法，这时为了检查随机梯度下降的收敛性，我们在每1000次迭代运算后，对最后1000个样本的cost值求一次平均，将这个平均值画到图中。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219205059495.png" alt="image-20250219205059495" style="zoom:33%;" /><p>下面是可能得到的几种图像：</p><p>图1：红色线的学习率比蓝色线要小，因此收敛的慢，最后收敛的更好一些。<br>图2：红线通过对5000次迭代求平均，而不是1000个，得到更加平滑的曲线。<br>图3：蓝线颠簸不平而且没有明显减少。可以增大$\alpha$来使得函数更加平缓，也许能使其像红线一样下降;或者可能仍像粉线一样颠簸不平且不下降，说明模型本身可能存在一些错误。<br>图4：如果曲线正在上升，说明算法发散。应该把学习速率$\alpha$的值减小。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219205613808.png" alt="image-20250219205613808" style="zoom:33%;" /><p>还可以令学习率随着迭代次数的增加而减小，例如令:$\alpha&#x3D;\frac{const1}{iterationNumber+const2}$, 这样，随着不断地靠近全局最小值，学习率会越来越小，迫使算法收敛而非在最小值附近徘徊。</p><p>但是通常不需要这样做便能有非常好的效果，对参数$const1, const2$进行调整比较麻烦。 </p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219211302129.png" alt="image-20250219211302129" style="zoom: 67%;" /><h2 id="17-5-在线学习"><a href="#17-5-在线学习" class="headerlink" title="17.5 在线学习"></a>17.5 在线学习</h2><p>有一种大规模的机器学习机制，叫做在线学习机制。让我们可以模型化问题。它指的是针对数据流，而非针对离线静态数据集进行学习。例如，许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能不将数据存储到数据库中，便顺利地进行算法学习。</p><p>在线学习的算法与随机梯度下降算法有些类似，只对单一的实例进行学习，而非对一个提前定义的训练集进行循环：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219213453785.png" alt="image-20250219213453785" style="zoom:67%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219212604841.png" alt="image-20250219212604841" style="zoom:33%;" /><p>一旦对一个数据的学习完成，便可以丢弃它，不需要再存储。这样的好处在于可以针对用户当前行为，不断更新模型以适应该用户。慢慢地调试学习到的假设，将其调节更新到最新的用户行为。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219213241968.png" alt="image-20250219213241968" style="zoom:33%;" /><h2 id="17-6-映射化简和数据并行"><a href="#17-6-映射化简和数据并行" class="headerlink" title="17.6 映射化简和数据并行"></a>17.6 映射化简和数据并行</h2><p>映射化简和数据并行对于大规模机器学习问题而言非常重要。之前提到，批量梯度下降算法计算代价非常大。如果能将数据集分配给多台计算机，让每一台计算机处理数据集的一个子集，然后将结果汇总求和，这样的方法叫做映射简化。</p><p>例如有 400 个训练实例，可以将批量梯度下降的求和任务分配给 4 台计算机进行处理:</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219214858928.png" alt="image-20250219214858928" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219215047820.png" alt="image-20250219215047820" style="zoom:33%;" /><p>如果任何学习算法能够表达为对训练集函数的求和，那么便能将这个任务分配给多台计算机(或者同一台计算机的不同 CPU 核心)，以达到加速处理的目的。例如逻辑回归：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219215358322.png" alt="image-20250219215358322" style="zoom:33%;" /><p>很多高级的线性代数函数库能够利用多核 CPU 来并行地处理矩阵运算，这也是算法的向量化实现如此重要的缘故(比调用循环快)。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219215758162.png" alt="image-20250219215758162" style="zoom:33%;" />]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>16.推荐系统</title>
    <link href="/2025/02/16/16%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    <url>/2025/02/16/16%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="16-推荐系统"><a href="#16-推荐系统" class="headerlink" title="16 推荐系统"></a>16 推荐系统</h1><h2 id="16-1-问题形式化"><a href="#16-1-问题形式化" class="headerlink" title="16.1 问题形式化"></a>16.1 问题形式化</h2><p>在机器学习领域，对于一些问题存在一些算法， 能试图自动地替你学习到一组优良的特征。通过推荐系统(recommender systems)，将领略一小部分特征学习的思想。</p><p>假使有 5 部电影，3部爱情片、2部动作片。 4 个用户为其中的部分电影打了分。现在希望构建一个算法，预测每个人可能给没看过的电影打多少分，以此作为推荐的依据。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218221557210.png" alt="image-20250218221557210" style="zoom:33%;" /><p>下面引入一些标记：</p><p>$n_u$：代表用户的数量<br>$n_m$：代表电影的数量<br>$r(i,j)$：如果用户 j 给电影 i 评过分则 $r(i,j) &#x3D; 1$<br>$y(i,j)$：代表用户 j 给电影 i 的评分 （注：这里 i 和 j 不要搞反）<br>$m_j$：代表用户 j 评过分的电影的总数</p><h2 id="16-2-基于内容的推荐系统"><a href="#16-2-基于内容的推荐系统" class="headerlink" title="16.2 基于内容的推荐系统"></a>16.2 基于内容的推荐系统</h2><p><strong>（1）定义</strong></p><p>在一个基于内容的推荐系统算法中，假设对于我们希望推荐的东西有一些数据，是这些东西的特征。<br>现在假设每部电影都有两个特征， $x_1$ 代表电影的浪漫程度，$x_2$ 代表电影的动作程度。</p><p>则每部电影都有一个特征向量，如 $x^{(1)}$是第一部电影的特征向量，为[1; 0.9; 0]。<br>下面我们采用线性回归模型，针对每一个用户都训练一个线性回归模型，如$\theta^{(1)}$ 是第一个用户的模型的参数。 于是有:<br>$\theta^{(j)}$  用户 j 的参数向量<br>$x^{(i)}$  电影 i 的特征向量<br>对于用户 j 和电影 i，我们预测其评分为：$(θ^{(j)})^Tx^{(i)}$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218222539890.png" alt="image-20250218222539890" style="zoom:33%;" /><p><strong>（2）代价函数</strong></p><p>对于单个用户 j，类比之前学习的线性回归和正则化，我们可以写出下面的代价函数，但这里我们去掉了$m^{(j)}$项(常数项不影响最终的参数$\theta^{(j)}$)，且不对方差项$\theta_0$进行正则化处理。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218223307103.png" alt="image-20250218223307103" style="zoom:33%;" /><p>上面的代价函数只是针对一个用户的，为了学习所有用户，我们将所有用户的代价函数求和:</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218223452211.png" alt="image-20250218223452211" style="zoom:33%;" /><p>如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式为:</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218223727853.png" alt="image-20250218223727853" style="zoom:33%;" /><h2 id="16-3-协同过滤"><a href="#16-3-协同过滤" class="headerlink" title="16.3 协同过滤"></a>16.3 协同过滤</h2><p>在之前的基于内容的推荐系统中，使用电影的特征，训练出了每一个用户的参数。相反地，如果拥有用户的参数，可以学习得出电影的特征。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218224911944.png" alt="image-20250218224911944" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218225200066.png" alt="image-20250218225200066" style="zoom:33%;" /><p>但是如果既没有用户的参数，也没有电影的特征，这两种方法都不可行了。可以使用协同过滤算法，同时学习这两者。</p><p>优化目标便改为同时针对$x$和$\theta$进行。是一个：预测$\theta$，再反过来预测$x$， 再预测$\theta$，再预测$x$的迭代过程。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218225439596.png" alt="image-20250218225439596" style="zoom:33%;" /><h2 id="16-4-协同过滤算法"><a href="#16-4-协同过滤算法" class="headerlink" title="16.4 协同过滤算法"></a>16.4 协同过滤算法</h2><p>通过分析前两节中的代价函数，我们可以发现粉色框表示对所有有评分的&lt;电影-用户&gt;对进行误差计算，我们可以将两个式子合并成最下面的一个式子，即为协同过滤算法的代价函数：</p><p>注:在协同过滤从算法中，通常不使用方差项，如果需要的话，算法会自动学得。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218230542742.png" alt="image-20250218230542742" style="zoom:33%;" /><p>算法流程：</p><p>1.初始化 $x$ 和 $\theta$ 为一些随机小值<br>2.使用梯度下降算法最小化代价函数 $J$，代价函数$J$的偏导数如下图<br>3.在训练完算法后，通过计算 $\theta^Tx$ 预测用户 j 给电影 i 的评分<br>通过这个学习过程获得的特征矩阵包含了有关电影的重要数据，这些数据不总是人能读懂的，但是可以用这些数据作为给用户推荐电影的依据。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218230841728.png" alt="image-20250218230841728" style="zoom:33%;" /><h2 id="16-5-向量化-低秩矩阵分解"><a href="#16-5-向量化-低秩矩阵分解" class="headerlink" title="16.5 向量化:低秩矩阵分解"></a>16.5 向量化:低秩矩阵分解</h2><p>协同过滤算法的向量化实现<br>举例:<br>1）给出一件产品，能否找到与之相关的其它产品。<br>2）一位用户最近看上一件产品，有没有其它相关的产品可以推荐给他。</p><p>现在有5部电影，4位用户，矩阵 Y 就是一个 5 行 4 列的矩阵，存储每个用户对每个电影的评分数据：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219194055498.png" alt="image-20250219194055498" style="zoom: 50%;" /><p>通过使用 $\theta$ 和 $x$​ 计算，可以预测出每个用户对每个电影打的分数：现在将所有 $x$ 都集中在一个大的矩阵$X$中，每一部电影是一行；将所有 $\theta$ 集中在一个大的$\Theta$中，每个用户是一行。则$X\Theta^T$矩阵中&lt;i, j&gt;元素即代表用户j对电影i评分的预测。</p><p>因为$X\Theta^T$矩阵，在数学上具有低秩属性。因此这个算法也被称为低秩矩阵分解 low rank matrix factorization。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219193550185.png" alt="image-20250219193550185" style="zoom:33%;" /><p>现在已经学习到了特征参数向量，那么可以使用这些向量做一些别的事情，比如度量两部电影之间的相似性。例如，如果一位用户正在观看电影 $x^{(i)}$ ，可以根据两部电影的特征向量之间的距离 $||x^{(i)}-x^{(j)}||$，寻找另一部相似电影 $x^{(j)}$：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219193928552.png" alt="image-20250219193928552" style="zoom:33%;" /><h2 id="16-6-推行工作上的细节-均值归一化-Mean-Normalization"><a href="#16-6-推行工作上的细节-均值归一化-Mean-Normalization" class="headerlink" title="16.6 推行工作上的细节:均值归一化 Mean Normalization"></a>16.6 推行工作上的细节:均值归一化 Mean Normalization</h2><p>现在新增一个用户 Eve，她没有为任何电影评分，那么我们以什么为依据为 Eve 推荐电影呢?</p><p>如果根据之前的模型，因为她没有打分，代价函数第一项为0。算法目标变为最小化最后一项，最后得到 $\theta^{(5)}$ 中的元素都是0。现在拿着 $\theta^{(5)}$ 预测出的评分都是0。这没有什么意义，因此需要做一些处理。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219195157732.png" alt="image-20250219195157732" style="zoom:33%;" /><p>首先需要对结果 Y 矩阵进行均值归一化处理，将每一个用户对某一部电影的评分(即整个评分矩阵)减去所有用户对该电影评分的平均值$u$，得到的新矩阵作为$Y$</p><p>然后利用这个新的 Y 矩阵来训练算法。 最后在预测评分时，需要在预测值的基础上加回平均值，即预测值等于 $(\theta^{(j)})^T(x^{(i)})+u_i$ 。因此对于 Eve，新模型预测出的她的打分都是该电影的平均分。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250219195818876.png" alt="image-20250219195818876" style="zoom:33%;" />]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>15.异常检测</title>
    <link href="/2025/02/16/15%20%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    <url>/2025/02/16/15%20%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="15-异常检测"><a href="#15-异常检测" class="headerlink" title="15 异常检测"></a>15 异常检测</h1><h2 id="15-1-异常检测问题的动机"><a href="#15-1-异常检测问题的动机" class="headerlink" title="15.1 异常检测问题的动机"></a>15.1 异常检测问题的动机</h2><p>异常检测算法虽然主要用于无监督学习问题，但从某些角度看，它又类似于一些监督学习问题。</p><p>举例： 当飞机引擎从生产线上流出时需要进行QA(质量控制测试)，数据集包含引擎的一些特征变量，比如运转时产生的热量，或者振动等。当有一个新的飞机引擎从生产线上流出，它具有特征变量 $x_{test}$ 。异常检测问题就是：希望知道这个新的飞机引擎是否有某种异常。如下图：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217223838714.png" alt="image-20250217223838714" style="zoom:33%;" /><p>训练出的模型，需要能够根据 $x_{test}$ 的位置告诉我们其属于一组数据的可能性 $p(x_{test})$。在下图中，蓝色圈内的数据属于该组数据的可能性较高；而越偏远，属于该组数据的可能性就越低。 这种方法称为密度估计，表达式如下：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217223806070.png" alt="image-20250217223806070" style="zoom:33%;" /><p>常见的异常检测问题如下：</p><p>例1：欺诈检测，通过 $p(x) &lt; \varepsilon$ 检测非正常用户。例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如:用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。根据这些特征构建一个模型，可以用来识别不符合该模式的用户。<br>例2：检测一个数据中心，特征可能包含:内存使用情况，被访问的磁盘数量，CPU 的负载，网络的通信量等。根据这些特征构建模型，用来判断某些计算机是否可能出错了。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217224254038.png" alt="image-20250217224254038" style="zoom:33%;" /><h2 id="15-2-高斯分布"><a href="#15-2-高斯分布" class="headerlink" title="15.2 高斯分布"></a>15.2 高斯分布</h2><p>高斯分布，也称为正态分布。如果变量 $x$ 符合高斯分布 $x∼N(u,\sigma^2)$ 则其概率密度函数如下 :</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217225310248.png" alt="image-20250217225310248" style="zoom:33%;" /><p>高斯分布样例如下图（其中 $u$ 决定中心点的位置， $\sigma^2$ 决定曲线的宽度）：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217225659959.png" alt="image-20250217225659959" style="zoom:33%;" /><p>通过已有数据集 $x$ ，可以预测总体的均值 $u$ 和方差 $\sigma^2$(参数估计) ，计算方法如下：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217230213719.png" alt="image-20250217230213719" style="zoom:33%;" /><p>机器学习中对于方差通常除以 m，而统计学中会除以(m − 1)。这两个公式在理论和数学特性上稍有不同，但在实际使用中的区别几乎可以忽略不计。</p><h2 id="15-3-异常检测算法"><a href="#15-3-异常检测算法" class="headerlink" title="15.3 异常检测算法"></a>15.3 异常检测算法</h2><p>现在我们应用高斯分布开发异常检测算法。 对于给定的数据集 $x$ ，针对<strong>每一个特征</strong>计算均值 $u$ 和方差 $\sigma^2$ 的估计值。然后，当出现一个新的训练实例，可以根据模型计算其对应的 $p(x)$，这个过程也叫做密度估计(Density estimation)，公式如下:</p><p>$p(x)&#x3D;\prod_{j&#x3D;1}^{n}{p(x_j;u_j,\sigma_j^2)}&#x3D;\prod_{j&#x3D;1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-u_j)^2}{2\sigma_j^2})$</p><p>（注：每个特征 $x_i$ 都对应不同的高斯分布）</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218194220268.png" alt="image-20250218194220268" style="zoom:33%;" /><p><strong>异常检测算法</strong>流程如下：</p><p>对于一个新的样本$x$，计算$p(x)$，若$p(x)&lt;\varepsilon$，则判断为异常。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218195252250.png" alt="image-20250218195252250" style="zoom:33%;" /><p>例子：下图中的 2D 图形是一个具有两个特征的训练集及其两个特征的分布情况，3D 图形表示密度估计函数，z轴为根据两个特征对$x_{test}$估计出的$p(x)$值。我们选择一个 $\varepsilon&#x3D;0.02$，将$p(x) &#x3D; \varepsilon$作为决策边界，当$p(x) &gt; \varepsilon$时预测为正常数据，否则为异常。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218200017243.png" alt="image-20250218200017243" style="zoom:33%;" /><h2 id="15-4-开发和评价一个异常检测系统"><a href="#15-4-开发和评价一个异常检测系统" class="headerlink" title="15.4 开发和评价一个异常检测系统"></a>15.4 开发和评价一个异常检测系统</h2><p>异常检测算法是一个无监督学习算法。但事实上，如果我们拥有一些带标记的数据，为了检验算法是否有效。可以在最开始将其看作一个监督学习算法。将已有数据分开，从中选择一部分正常数据作为训练集，剩下的正常数据和异常数据混合构成交叉检验集和测试集。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218200611823.png" alt="image-20250218200611823" style="zoom:33%;" /><p>例如：有 10000 台正常引擎的数据，有 20 台异常引擎的数据。 </p><p>我们这样分配数据:</p><p>6000 台正常引擎的数据作为Training set；<br>2000 台正常引擎和 10 台异常引擎的数据作为CV set<br>2000 台正常引擎和 10 台异常引擎的数据作为Test set</p><p>但还有一些人把同样一组数据既用作CV集，也用作Test集。这是不好的做法。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218201108538.png" alt="image-20250218201108538" style="zoom:33%;" /><p>算法流程：</p><p>1） 根据训练集数据，估计特征的平均值和方差并构建$p(x)$函数<br>2） 对CV set，尝试使用不同的 $\varepsilon$ 值作为阈值，并预测数据是否异常，根据 F1 值或者查准率与查全率的比例来选择 $\varepsilon$<br>3） 选出 $\varepsilon$ 后，针对Test集进行预测，计算异常检验系统的F1值，或者查准率与查全率之比</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218201856617.png" alt="image-20250218201856617" style="zoom:33%;" /><h2 id="15-5-异常检测与监督学习对比"><a href="#15-5-异常检测与监督学习对比" class="headerlink" title="15.5 异常检测与监督学习对比"></a>15.5 异常检测与监督学习对比</h2><p>之前构建的异常检测系统也使用了带标记的数据，与监督学习有些相似。下面对异常检测和监督学习进行对比:</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218203847899.png" alt="image-20250218203847899" style="zoom: 50%;" /><h2 id="15-6-选择特征"><a href="#15-6-选择特征" class="headerlink" title="15.6 选择特征"></a>15.6 选择特征</h2><p><strong>1.特征转换</strong></p><p>特征的选择对异常检测算法至关重要。假设特征不符合高斯分布，算法也能够工作，但最好还是将数据转换成高斯分布，例如： 使用对数函数 $x &#x3D; log(x + c)$，其中 c 为非负常数; 或者 $x &#x3D; x^c$ ，c 为 0-1 之间的一个分数。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218204604461.png" alt="image-20250218204604461" style="zoom:33%;" /><p>如下图，一些异常的数据可能也会有较高的$p(x)$值，因而被算法认为是正常的。 可以使用误差分析帮我们分析是否存在问题。也许从问题样本中发现需要增加一些新的特征$x_2$，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218205334657.png" alt="image-20250218205334657" style="zoom:33%;" /><p>通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征(异常数据的该特征值异常地大或小)。例如增加两个特征值的比例。</p><p>例如，在检测计算机状况的例子中，可以用 CPU负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器是陷入了一些问题中(死循环中，CPU负载高，网络通信量低)。如下图：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218205855273.png" alt="image-20250218205855273" style="zoom:33%;" /><h2 id="15-7-多元高斯分布"><a href="#15-7-多元高斯分布" class="headerlink" title="15.7 多元高斯分布"></a>15.7 多元高斯分布</h2><p><strong>（1）多元高斯分布的定义</strong></p><p>假如我们有两个相关的特征，其值域范围比较宽。一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去分别抓住两个特征的偏差，得到的判定边界范围比较大。<br>例如下图中是两个相关特征，粉色的线(根据 $\varepsilon$ 的不同其范围可大可小)是原始高斯分布模型获得的判定边界，绿色的 X 点很可能是异常值，但其$p(x_1)$、$p(x_2)$值却仍然在正常范围内。</p><p>如果使用多元高斯分布，获得蓝色曲线所示的判定边界，范围更小，判定结果会更准确。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218210520583.png" alt="image-20250218210520583" style="zoom:33%;" /><p>回顾下一般的高斯分布模型，通过分别计算每个特征对应的几率，将其累乘起来，得到 $p(x)$。</p><p>$p(x)&#x3D;\prod_{j&#x3D;1}^{n}{p(x_j;u_j,\sigma_j^2)}&#x3D;\prod_{j&#x3D;1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-u_j)^2}{2\sigma_j^2})$</p><p>而多元高斯分布模型将构建特征的协方差矩阵，使用所有的特征一次性计算出 p(x)。 首先，计算所有特征的平均值，然后再计算协方差矩阵:</p><p>$u&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}x^{(i)}$</p><p>$\Sigma&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}(x^{(i)}-u)(x^{(i)}-u)^T&#x3D;\frac{1}{m}(X-u)^T(X-u)$</p><p>$p(x)&#x3D;\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-u)^T\Sigma^{-1}(x-u))$其中： $|\Sigma|$是矩阵$\Sigma$的行列式，在 Matlab 中用det(sigma) 计算； $\Sigma^{-1}$ 是逆矩阵。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218211118954.png" alt="image-20250218211118954" style="zoom:33%;" /><p><strong>（2）协方差矩阵对模型的影响</strong></p><p>协方差主对角线上的值影响$x_1$和$x_2$图像的方差，值越大方差越大；负对角线上的值影响$x_1$和$x_2$之间的关系（正值为正相关，负值为负相关）。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218212309853.png" alt="image-20250218212309853" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218212428453.png" alt="image-20250218212428453" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218212643927.png" alt="image-20250218212643927" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218212743599.png" alt="image-20250218212743599" style="zoom:33%;" /><p><strong>（3）均值$u$对中心点的影响</strong></p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218212904188.png" alt="image-20250218212904188" style="zoom:33%;" /><h2 id="15-8-使用多元高斯分布进行异常检测"><a href="#15-8-使用多元高斯分布进行异常检测" class="headerlink" title="15.8 使用多元高斯分布进行异常检测"></a>15.8 使用多元高斯分布进行异常检测</h2><p><strong>（1）使用多元高斯分布进行异常检测</strong></p><p>算法步骤如下：</p><p>1）代入参数估计的公式，计算出均值 $u$ 和协方差矩阵 $\Sigma$</p><p>2）对新实例 $x$， 根据公式计算其 $p(x)$ 的值，如果小于 $\varepsilon$ 则异常。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218213823135.png" alt="image-20250218213823135" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218214223376.png" alt="image-20250218214223376" style="zoom:33%;" /><p><strong>（2）原始高斯分布模型 和 多元高斯分布模型</strong></p><p>可以看出：对于一个多元高斯分布模型，如果其协方差矩阵只有正对角线上元素非零，则退化为原始高斯分布模型。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218214613456.png" alt="image-20250218214613456" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218215958109.png" alt="image-20250218215958109" style="zoom:50%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250218215623133.png" alt="image-20250218215623133" style="zoom:33%;" /><p>若协方差矩阵$\Sigma$不可逆，通常是没有满足$m&gt;n$或者存在特征冗余。</p><p>总结：<br>原高斯分布模型被广泛使用，如果特征之间在某种程度上相互关联，可以通过构造新特征的方法来捕捉这些相关性。<br>通常$m\ge10n$时，并且没有太多的特征（n不是太大），可以使用多元高斯分布模型。</p>]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>14.降维</title>
    <link href="/2025/02/16/14%20%E9%99%8D%E7%BB%B4/"/>
    <url>/2025/02/16/14%20%E9%99%8D%E7%BB%B4/</url>
    
    <content type="html"><![CDATA[<h1 id="14-降维"><a href="#14-降维" class="headerlink" title="14 降维"></a>14 降维</h1><h2 id="14-1-降维的动机一：数据压缩"><a href="#14-1-降维的动机一：数据压缩" class="headerlink" title="14.1 降维的动机一：数据压缩"></a>14.1 降维的动机一：数据压缩</h2><p>下面介绍第二种无监督学习问题：降维。降维的一个作用是数据压缩，可以减小数据占用内存和磁盘的空间，还可以加快算法速度。</p><p>例如：假设我们用两个特征来描述同一个物体的长度，$x_1$的单位是厘米，$x_2$的单位是英尺。这将导致高度冗余，所以需要进行降维到一维。我们将二维的数据点$x^{(i)}&#x3D;[x_1^{(i)};x_2^{(i)}]\in\mathbb{R}^2$投影到绿色的直线上，根据各点在投影直线上的位置$z^{(i)}\in\mathbb{R}$作为新特征完成降维。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216214729237.png" alt="image-20250216214729237" style="zoom:33%;" /><p>将数据从三维降至二维：观察数据点的分布大都集中在一个二维平面上，我们可以将三维向量投影到一个二维平面上，降至二维的特征向量。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216215642567.png" alt="image-20250216215642567" style="zoom:33%;" /><p>事实工作中，不同的团队可能会给你几百或成千上万的特征，其中很容易出现冗余特征。这时候就可以使用降维减少特征冗余。</p><h2 id="14-2-降维的动机二：数据可视化"><a href="#14-2-降维的动机二：数据可视化" class="headerlink" title="14.2 降维的动机二：数据可视化"></a>14.2 降维的动机二：数据可视化</h2><p>降维的另一个作用是数据可视化，可以帮助我们优化学习算法。</p><p>例如：我们有关于每个国家的数据，特征向量$x^{(i)}\in\mathbb{R}^{50}$，50维的数据是不好可视化，不容易进行数据分析的，将其降至2维，便可以将其可视化了。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216221800695.png" alt="image-20250216221800695" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216222130349.png" alt="image-20250216222130349" style="zoom:33%;" /><p>将数据降维至低维度，这样我们就可以将数据进行可视化，并更好的理解这些数据。但这样做新特征的意义通常需要由我们自己去发现。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216222403494.png" alt="image-20250216222403494" style="zoom:33%;" /><h2 id="14-3-PCA问题的公式描述"><a href="#14-3-PCA问题的公式描述" class="headerlink" title="14.3 PCA问题的公式描述"></a>14.3 PCA问题的公式描述</h2><p>对于降维算法，比较常见的是主成分分析方法（PCA）。</p><p>例如，我们有二维的特征向量，我们想把它降维至一维。即我们要找一条投影直线完成降维。PCA做的就是找一个低维的直线&#x2F;平面&#x2F;其他维的子空间，然后将数据投影在上面，使蓝色线段长度平方最小。这些蓝色线段长度也被称为投影误差。如图：PCA会更倾向于找到红色的直线，因为相比于粉色直线，红色线的投影误差更小。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216223101545.png" alt="image-20250216223101545" style="zoom:33%;" /><p>PCA问题：将n维数据降至k维，目标是找到向量$u^{(1)},u^{(2)},…,u^{(k)}$，将数据点投影到上面，使得总的投影误差最小。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216223810187.png" alt="image-20250216223810187" style="zoom:33%;" /><p>PCA 与 线性回归 的比较：</p><p>PCA最小化的是投射误差（垂直距离），不作任何预测。线性回归最小化的是预测误差（竖直距离），目的是预测结果。如图：左边的是线性回归的误差(垂直于横轴投影，预测值与真实标签的差值)，右边则是主要成分分析的误差(垂直于斜线投影)。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216224416786.png" alt="image-20250216224416786" style="zoom:33%;" /><h2 id="14-4-PCA算法"><a href="#14-4-PCA算法" class="headerlink" title="14.4 PCA算法"></a>14.4 PCA算法</h2><p>在进行PCA算法之前，需要数据预处理，进行特征缩放&#x2F;均值归一化。</p><p><strong>1）均值归一化</strong></p><p>计算出所有特征的均值，然后令 $x_j &#x3D; x_j − u_j$ 。如果特征是在不同的数量级上，还需要将其除以标准差 $\sigma^2$ 。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217195243781.png" alt="image-20250217195243781" style="zoom:33%;" /><p>对于PCA算法，我们需要找到一个低维子空间，用来投影数据。那么如何找到低维子空间的向量$u^{(1)},u^{(2)}…$以及新特征向量$z$呢？</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217200035262.png" alt="image-20250217200035262" style="zoom:33%;" /><p><strong>2）计算协方差矩阵sigma$\Sigma$</strong></p><p>这里协方差矩阵$\Sigma\in\mathbb{R}^{n\times n}$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217200727078.png" alt="image-20250217200727078" style="zoom:33%;" /><p><strong>3）计算协方差矩阵$\Sigma$的特征向量</strong></p><p>我们可以利用奇异值分解(singular value decomposition)来得到特则向量矩阵 U，调用方式为 [U， S，V] &#x3D; svd(sigma) 。我们想要从n维降至k维，那么就取矩阵U的前k列作为一个新的矩阵$U_{reduce}\in\mathbb{R}^{n\times k}$，我们的新特征向量$z^{(i)}&#x3D;(U_{reduce})^Tx^{(i)}$, $z^{(i)}\in \mathbb{R}^{k\times 1}$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217201658366.png" alt="image-20250217201658366" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217202819199.png" alt="image-20250217202819199" style="zoom:33%;" /><h2 id="14-5-重建原始特征"><a href="#14-5-重建原始特征" class="headerlink" title="14.5 重建原始特征"></a>14.5 重建原始特征</h2><p>如何从压缩后的低维表示$z^{(i)}$映射回原来高维的数据$x^{(i)}$呢？即如何根据压缩后的数据去重建原始数据?</p><p>例如：原始数据$x^{(i)}$是二维，$z^{(i)}$是一维，$z&#x3D;U_{reduce}^Tx$，则相反的方程为$x_{approx}&#x3D;U_{reduce}z$，这里的$x_{approx}$是投影之后的点，通过此方程可以得到投影点的坐标。又因为$x_{approx}\approx x$，所以可以通过此方法得到原始数据的一个近似表示。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217212551347.png" alt="image-20250217212551347" style="zoom:33%;" /><h2 id="14-6-选择主成分的数量k"><a href="#14-6-选择主成分的数量k" class="headerlink" title="14.6 选择主成分的数量k"></a>14.6 选择主成分的数量k</h2><p>主成分分析最小化投射的平均均方误差，怎么选择适当降维目标 k 值（即主成分的数量）呢？我们的目标是：<strong>在『平均均方误差与总变差的比例尽可能小』的情况下，选择尽可能小的 k 值。</strong></p><p>我们定义平均均方投影误差，总变差（也可以理解为样本和全零点之间的距离）如下：如果希望比例小于 1%， 就意味着原本数据的偏差有 99% 都保留下来了。<br>通常95%到99%是最常用的取值范围。（注：对于许多数据集，通常可以在保留大部分差异性的同时大幅降低数据的维度。这是因为大部分现实数据的许多特征变量都是高度相关的。）</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217204202445.png" alt="image-20250217204202445" style="zoom:33%;" /><p>具体的做法是：</p><p>我们可以从k&#x3D;1开始，然后进行主成分分析，计算得到$U_{reduce}$和$z$，然后计算比例是否小于0.01。若不是令k&#x3D;2，重复此过程，直至找到可以让比例小于0.01的最小的k值。</p><p>但上面的方法是低效的。还有更好的方法，我们进行奇异值分解时返回了一个矩阵$S\in\mathbb{R}^{n\times n}$, 矩阵$S$是一个对角矩阵，由$\Sigma$的特征值组成，对角线上的特征值非负且按降序排列。因为我们的目的是从 n 维降到 k 维，也就是选出这 n 个特征中最重要的 k 个，也就是选出特征值最大的 k 个。所以得到矩阵 S 后，我们可以直接用它来计算平均均方误差与总变差的比例。如右图公式，每次改变k，只需要重新计算分子即可:</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217205504520.png" alt="image-20250217205504520" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217211046671.png" alt="image-20250217211046671" style="zoom:33%;" /><h2 id="14-7-PCA的应用建议"><a href="#14-7-PCA的应用建议" class="headerlink" title="14.7 PCA的应用建议"></a>14.7 PCA的应用建议</h2><p>假设我们有m张100*100像素的图片，即特征向量$x^{(i)}\in\mathbb{R}^{10000}$，使用PCA算法的步骤如下：1.使用PCA算法将特征向量$x^{(i)}$压缩到1000个特征。2.对压缩后的训练集运行学习算法。3.在预测时，采用之前学习得到的 $U_{reduce}$ 将输入的特征 $x$ 转换成特征向量 $z$ ，然后再进行预测.</p><p>注意：映射关系只能在训练集上运行PCA学习得到，验证集和测试集也采用对训练集学习得来的$U_{reduce}$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217214233137.png" alt="image-20250217214233137" style="zoom:33%;" /><p>正确用法：下图总结了PCA的应用以及在各应用上如何选择k的值：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217215715285.png" alt="image-20250217215715285" style="zoom:33%;" /><p>补充：PCA可以加快算法运行速度。</p><p>错误用法：</p><p>1.使用PCA减少特征的数量，从而用于减少过拟合。</p><p>这样做非常不好的，不如使用正则化处理。原因在于 PCA 只是近似地丢弃掉一些特征，它并不考虑任何与结果变量$y$有关的信息，因此可能会丢失非常重要的特征。而当进行正则化处理时，会考虑到结果变量$y$，不会丢掉重要的数据。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217220307209.png" alt="image-20250217220307209" style="zoom:33%;" /><p>2.在项目开始时，就将PCA考虑在项目计划中。</p><p>最好还是从所有原始特征开始，只在有必要的时候(算法运行太慢或者占用太多内存)才考虑采用 PCA。因为PCA会影响模型精确度。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250217220725755.png" alt="image-20250217220725755" style="zoom:33%;" />]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>13.聚类</title>
    <link href="/2025/02/16/13%20%E8%81%9A%E7%B1%BB/"/>
    <url>/2025/02/16/13%20%E8%81%9A%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="13-聚类"><a href="#13-聚类" class="headerlink" title="13 聚类"></a>13 聚类</h1><h2 id="13-1-无监督学习简介"><a href="#13-1-无监督学习简介" class="headerlink" title="13.1 无监督学习简介"></a>13.1 无监督学习简介</h2><p>在无监督学习中，我们的训练样本不包含任何标签$y$。算法将从训练样本中找到一些隐含在数据中的结构。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216155313379.png" alt="image-20250216155313379" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216155405876.png" alt="image-20250216155405876" style="zoom:33%;" /><h2 id="13-2-K-Means算法"><a href="#13-2-K-Means算法" class="headerlink" title="13.2 K-Means算法"></a>13.2 K-Means算法</h2><p>在聚类算法中，算法将无标签的数据划分成有紧密关系的簇。</p><p>K-Means算法是最热门的聚类算法，K-Means算法接收2个输入，分别是K（要划分的簇的个数）、无标签的训练数据集。算法过程如下：<br>1)随机初始化聚类中心：随机选取K个点，称为聚类中心。<br>2)分配到簇：对于m个训练样本，计算到K个中心点的距离，然后将其划分到距离最近的中心点对应的簇中。即$c^{(i)}:&#x3D;\underset{k}{min}||x^{(i)}-u_k||^2,k\in[1,K]$<br>3)移动聚类中心：对于K个簇，分别计算每一个簇中的点的平均值，将该簇所关联的中心点移动到平均值的位置。这里的$u_k$是一个n维向量。(对于没有点的簇，可以直接删除该中心点或者重新初始化中心点)<br>4)重复2-3步骤，直至中心点不再变化。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216160451739.png" alt="image-20250216160451739" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216161353157.png" alt="image-20250216161353157" style="zoom:33%;" /><p>下面是一个K-Means算法的示例：</p><p>初始化随机的中心点，计算距离后分类，然后移动中心点</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216164842116.png" alt="image-20250216164842116" style="zoom: 67%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216164901926.png" alt="image-20250216164901926" style="zoom:67%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216164933177.png" alt="image-20250216164933177" style="zoom:67%;" /><p>多次迭代后，最终的聚类结果如下：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216165029983.png" alt="image-20250216165029983" style="zoom:50%;" /><p>在没有非常明显组群的情况下，也可以使用K-Means：对于右图是不同人的身高体重，我们想要设计S、M、L三种型号的衬衫，那么就可以将数据划分成3类，每类就对应一种衬衫型号。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216163554320.png" alt="image-20250216163554320" style="zoom:33%;" /><h2 id="13-3-优化目标"><a href="#13-3-优化目标" class="headerlink" title="13.3 优化目标"></a>13.3 优化目标</h2><p>了解K-Means算法的代价函数有助于理解算法过程，也可以借助它帮助K-Means算法找到更好的簇，并且避免局部最优解。</p><p>K-Means的代价函数为：$J(c^{(1)},…,c^{(m)},u_1,…,u_K)&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}||x^{(i)}-u_{c^{(i)}}||^2$.我们要找到参数$c^{(1)},…,c^{(m)}$, $u_1,…,u_K$使得代价函数$J$最小。即要最小化所有数据点与其所关联的聚类中心点之间的距离之和。该代价函数也称为失真代价函数（distortion cost function）</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216170536157.png" alt="image-20250216170536157" style="zoom:33%;" /><p>可以证明：K-Means算法中分配到簇的步骤就是在最小化$J$关于变量$c$的代价，移动聚类中心就是在最小化$J$关于变量$u$的代价。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216171754415.png" alt="image-20250216171754415" style="zoom:33%;" /><h2 id="13-4-随机初始化"><a href="#13-4-随机初始化" class="headerlink" title="13.4 随机初始化"></a>13.4 随机初始化</h2><p>随机初始化聚类中心点的方法：1.选择K&lt;m，即聚类中心点的个数要小于训练样本的数量 2.随机选择K个训练样本作为聚类中心。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216200000313.png" alt="image-20250216200000313" style="zoom:33%;" /><p>随机初始化的状态不同，K-Means最终可能会得到不同的结果。如果初始化不好，有可能会停留在一个局部最优处（如下图右下角两种局部最优的情况）。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216201028721.png" alt="image-20250216201028721" style="zoom:33%;" /><p>为了避免算法陷入局部最优解，我们可以运行多次K-Means算法（通常运行50<del>1000次），每一次都重新随机初始化，最后比较多次运行 K-Means的结果，选择代价函数最小的结果。这种方法在K较小的时候(K&#x3D;2</del>10)可行，如果K较大可能不会有明显地改善。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216201341304.png" alt="image-20250216201341304" style="zoom:33%;" /><h2 id="13-5-选择聚类数量"><a href="#13-5-选择聚类数量" class="headerlink" title="13.5 选择聚类数量"></a>13.5 选择聚类数量</h2><p>没有最好的选择聚类数的方法，通常是需要根据不同的问题人工选择。需要思考运用 K-Means算法的动机，然后选择能最好服务于该目的的聚类数。</p><p>方法一：肘部法则。通过改变聚类数K，画出代价函数$J$随K值变化的曲线，可能会得到一条类似肘部的曲线。如左图，在$K&#x3D;3$的左侧代价函数$J$下降较快，而在$K&#x3D;3$的右侧代价函数$J$下降缓慢。$K&#x3D;3$这个点就是肘点，在此之后，代价函数$J$就下降的非常慢，那么我们就选K &#x3D; 3。</p><p>但有时也会出现右图的情况，代价函数曲线比较平滑，没有肘点。这时就需要人工选择。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216211801393.png" alt="image-20250216211801393" style="zoom:33%;" /><p>方法二：考虑下游任务的目的。通常聚类算法服务于下游任务，可以根据下游任务的目的，看哪个参数K能更好地应用于下游任务。例如：是生产5种尺寸的衬衫让衬衫更合身，还是生产3种尺寸的衬衫让衬衫价格更便宜。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216213025911.png" alt="image-20250216213025911" style="zoom:33%;" />]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>12.支持向量机</title>
    <link href="/2025/02/16/12%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <url>/2025/02/16/12%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="12-支持向量机"><a href="#12-支持向量机" class="headerlink" title="12 支持向量机"></a>12 支持向量机</h1><h2 id="12-1-优化目标"><a href="#12-1-优化目标" class="headerlink" title="12.1 优化目标"></a>12.1 优化目标</h2><p>我们通过回顾逻辑回归，一步步将其修改为SVM。在逻辑回归中，若一个样本对应的$y&#x3D;1$，我们就想让$h_\theta(x) \approx1$, 也就是让$\theta^Tx &gt;&gt;0$. 若一个样本对应的$y&#x3D;0$，我们就想让$h_\theta(x) \approx0$, 也就是让$\theta^Tx &lt;&lt;0$. </p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210194748828.png" alt="image-20250210194748828" style="zoom:33%;" /><p>只考虑一个样本$(x,y)$时，代价函数如下所示：若$y&#x3D;1$，代价函数关于$z$的曲线如左图中蓝色曲线；若$y&#x3D;0$，代价函数关于$z$的曲线如右图中蓝色曲线。在SVM中，我们分别用粉色的线段作为新的代价函数。左边的函数称为$cost_1(z)$，右边函数称为 $cost_0(z)$。在之后的优化问题中，这种形式的 cost function 会为 SVM 带来计算上的优势。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210195532293.png" alt="image-20250210195532293" style="zoom:33%;" /><p>SVM的代价函数可以从逻辑回归的代价函数演变而来。主要有3个变化：1.使用之前定义的 $cost_1()$ 和 $cost_0()$ 替换公式中对应的项。2.去掉了$\frac{1}{m}$这个系数（因为对于最优化问题，乘除一个常数是不影响我们得出同样的$\theta$最优值）3.化为$C\times A+B$的形式。（对于逻辑回归，代价函数为$ A +\lambda\times B $，通过设置不同的$\lambda$ 来权衡$A$与$B$的重要性。 对于SVM， 我们删掉$\lambda$，引入常数$C$， 将代价函数改为$ C\times A + B$， 通过设置不同的$ C $达到优化目的。在优化过程中，其意义和逻辑回归是一样的。可以理解为$ C &#x3D; 1 &#x2F; \lambda$）</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210200613747.png" alt="image-20250210200613747" style="zoom:33%;" /><p>得到SVM的代价函数为：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210202015324.png" alt="image-20250210202015324" style="zoom:33%;" /><p>另外，逻辑回归中假设的输出是一个概率值。 而 SVM 直接预测$ y &#x3D; 1$，还是$ y &#x3D; 0$。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210202130848.png" alt="image-20250210202130848" style="zoom:33%;" /><h2 id="12-2-大间距分类的直观理解-Large-Margin-Intuition"><a href="#12-2-大间距分类的直观理解-Large-Margin-Intuition" class="headerlink" title="12.2 大间距分类的直观理解 Large Margin Intuition"></a>12.2 大间距分类的直观理解 Large Margin Intuition</h2><p>SVM又被称为大间距分类器</p><p>在之前的定义中，只要$\theta^Tx \ge0$就被分为正类，只要$\theta^Tx &lt;0$就被分为负类. 但通过观察SVM的代价函数曲线，发现若$y&#x3D;1$，我们想要让$\theta^Tx \ge1$，这样才能让$cost_1(z)&#x3D;0$。同样，若$y&#x3D;0$，我们想要让$\theta^Tx \le-1$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210203952963.png" alt="image-20250210203952963" style="zoom:33%;" /><p>当超参数$C$很大时，代价函数就会尽量让第一项等于0. 也就是让：若$y^{(i)}&#x3D;1$，让$\theta^Tx^{(i)}\ge1$​；若$y^{(i)}&#x3D;0$，让$\theta^Tx^{(i)}\le-1$。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210205526451.png" alt="image-20250210205526451" style="zoom:33%;" /><p>这样SVM会把正负样本以最大的间距margin分开，因此SVM有着更好的鲁棒性。黑线即为SVM分类器。只有当 C 特别大的时候， SVM 才是一个最大间隔分类器。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210205139228.png" alt="image-20250210205139228" style="zoom:33%;" /><p>1.如果想将样本用最大间距分开，即将 C 设置的很大。那么仅因为一个异常点，决策边界会从黑线变成那条粉线，这实在是不明智的。2.如果 C 设置的小一点，最终得到这条黑线。它可以忽略一些异常点的影响，而且当数据线性不可分的时候，也可以将它们恰当分开，得到更好地决策边界。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210211208567.png" alt="image-20250210211208567" style="zoom:33%;" /><h2 id="12-3-大间距分类器背后的数学原理"><a href="#12-3-大间距分类器背后的数学原理" class="headerlink" title="12.3 大间距分类器背后的数学原理"></a>12.3 大间距分类器背后的数学原理</h2><p>向量的模长（范数）$||u||&#x3D;\sqrt{u_1^2+u_2^2}\in \mathbb{R}$. </p><p>向量内积$u^Tv&#x3D;||u||\cdot||v||\cdot cos\theta&#x3D;||u||\cdot p\in \mathbb{R}$.（这里$p$是向量$v$在向量$u$上投影的长度，是一个数，$uv$同向为正，反向为负）.</p><p>根据线代公式$u^Tv&#x3D;u_1v_1+u_2v_2&#x3D;v^Tu$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210212402642.png" alt="image-20250210212402642" style="zoom:33%;" /><p>这节将解释为什么当 C 特别大的时候， SVM 会是一个最大间隔分类器？</p><p>首先我们假设C很大，$\theta_0&#x3D;0,n&#x3D;2$(只有两个特征)。C很大时，我们的优化目标是$min \frac{1}{2}\sum_{j&#x3D;1}^{n}\theta_j^2&#x3D;\frac{1}{2}||\theta||^2$. 我们再来看$\theta^Tx^{(i)}&#x3D;p^{(i)}||\theta||&#x3D;\theta_1x_1^{(i)}+\theta_2x_2^{(i)}$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210214418218.png" alt="image-20250210214418218" style="zoom:33%;" /><p>根据上面的内积公式，得到：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210220023188.png" alt="image-20250210220023188" style="zoom:33%;" /><p>SVM如何选择更优的决策边界？</p><p>假设我们得到了一条绿色的决策边界。因为$\theta$为边界函数的系数，所以以$\theta$为元素的向量和以$\theta$为系数的直线垂直.样本$x^{(1)}$在向量$\theta$上的投影为红色$p^{(1)}$, 样本$x^{(2)}$在向量$\theta$上的投影为粉色$p^{(2)}$.</p><p>对于正样本$x^{(1)}$而言，想要$p^{(1)}||\theta||\ge1$，现在$p^{(1)}$长度非常短，就意味着$||\theta||$需要非常大。对于负样本$x^{(2)}$而言，想要$p^{(2)}||\theta||\le-1$，现在$p^{(2)}$长度非常短，就意味着$||\theta||$需要非常大。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210220235324.png" alt="image-20250210220235324" style="zoom:50%;" /><p>但我们的目标函数$min\frac{1}{2}\sum_{j&#x3D;1}^{n}\theta_j^2&#x3D;\frac{1}{2}||\theta||^2$是希望最小化参数$\theta$的范数，因此我们希望： 投影长度 $p^{(i)}$ 尽可能大。例如下面这条绿色的决策边界，就更好一些：</p><p>可以发现此时$p^{(1)}$更大了，想要$p^{(1)}||\theta||\ge1$，$||\theta||$就可以变小了。达到了减低损失函数的目的。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250210221533509.png" alt="image-20250210221533509" style="zoom: 50%;" /><p>总结：<strong>SVM希望正负样本投影到$\theta$的值$p^{(i)}$足够大</strong>，实际上这些$p^{(i)}$值就等于间隔margin，<strong>也就是最大化了间隔</strong>，最终SVM找到了较小的$\theta$的范数，也就是为什么最终找到了大间隔分类器。</p><h2 id="12-4-核函数1"><a href="#12-4-核函数1" class="headerlink" title="12.4 核函数1"></a>12.4 核函数1</h2><p>接下来我们改造SVM来完成非线性分类问题</p><p>我们可以使用高阶多项式来解决非线性分类问题。可以用一系列新的特征 f 来替换模型中的每一项。例如可以令$f_1&#x3D;x_1,f_2&#x3D;x_2,f_3&#x3D;x_1x_2,f_4&#x3D;x_1^2$, 得到$h_\theta(x)&#x3D;\theta_0+\theta_1f_1+\theta_2f_2+…$。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211200201889.png" alt="image-20250211200201889" style="zoom:33%;" /><p>特征$f$的选取是多样的，有没有更好的方法来构造特征$f$呢？我们可以利用核函数来计算出新的特征。</p><p>我们手动选取一些标记landmarks，分别为$l^{(1)},l^{(2)},l^{(3)}$. 给定一个训练实例$x$，我们用$x$与预先选定的标记 $l^{(1)} ， l^{(2)} ， l^{(3)}$ 的相似程度（核函数）作为新的特征$ f_1 ， f_2 ， f_3$，这里我们使用了高斯核函数。特征$ f_1 ， f_2 ， f_3$的表达式如下：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211200114480.png" alt="image-20250211200114480" style="zoom:33%;" /><p>因为$||x-l^{(1)}||^2&#x3D;\sum_{j&#x3D;1}^n(x_j-l_j^{(1)})^2$（忽略了$x_0&#x3D;0$）.如果训练实例$x$与$l^{(1)}$很近，则$f_1\approx1$；如果训练实例$x$与$l^{(1)}$很远，则$f_1\approx0$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211202225630.png" alt="image-20250211202225630" style="zoom:33%;" /><p>假设$x$含有两个特征$[x_1,x_2]$，不同的$\sigma$值会有不同效果。 图中水平面的坐标为 $x_1， x_2$ , 而垂直坐标轴代表 $f_1$。只有当 $x$ 与 $l^{(1)}$ 重合时， $f_1$ 才具有最大值。特征$f_1$衡量了$x$到标记$l^{(1)}$有多近。随着 $x$ 的改变 $f_1$ 值的变化速率受到 $\sigma^2$ 的控制。 $\sigma^2$越小，曲线越瘦.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211203327359.png" alt="image-20250211203327359" style="zoom:33%;" /><p>假设我们已学到的参数$\theta&#x3D;[-0.5;1;1;0]$。下图中粉色的点离 $l^{(1)}$ 更近，所以 $f_1$ 接近 1，而 $f_2$ ，$f_3$ 接近 0。因此 $h_\theta(x) \ge 0$，因此预测 $y &#x3D; 1$；同理，绿色点离 $l^{(2)}$ 较近，也预测$y &#x3D; 1$；但浅蓝色点离 $l^{(1)}$, $l^{(2)}$ 都较远，预测$y &#x3D; 0$。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211204243433.png" alt="image-20250211204243433" style="zoom:33%;" /><p>距离 landmarks 近预测为1，距离远预测为0. 图中红色封闭曲线便是决策边界。在预测时我们采用的特征不是训练实例本身的特征，而是通过标记点和核函数计算出的新特征$f_1$ ， $f_2$ ， $f_3$ 。从而训练出了复杂的非线性决策边界.</p><h2 id="12-5-核函数2"><a href="#12-5-核函数2" class="headerlink" title="12.5 核函数2"></a>12.5 核函数2</h2><p><strong>（1）怎么选取landmarks呢？</strong></p><p>我们将每一个样本都作为一个landmark。即$l^{(i)}&#x3D;x^{(i)}$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211210053326.png" alt="image-20250211210053326" style="zoom:33%;" /><p>对于训练样本$(x^{(i)},y^{(i)})$，我们可以计算出特征向量$f^{(i)}&#x3D;[f_0^{(i)};f_1^{(i)};…;f_m^{(i)}]\in \mathbb{R}^{m+1}$. 最后，若$\theta^Tf^{(i)}\ge0$，预测为1.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211210936084.png" alt="image-20250211210936084" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211212700639.png" alt="image-20250211212700639" style="zoom:33%;" /><p><strong>（2）将核函数kernel引入SVM</strong></p><p>将高斯核函数代入SVM的代价函数中，如下图。这里与之前的代价函数的区别在于用核函数$f$代替了$x$。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211213210372.png" alt="image-20250211213210372" style="zoom:33%;" /><p><strong>（3）简化计算</strong></p><p>为了简化计算， 在计算正则项 $\theta^T\theta$ 时，用 $\theta^TM\theta$ 代替 $\theta^T\theta$ ，其中 M 是一个矩阵，核函数不同则M不同。<br>（注：理论上也可以在逻辑回归中使用核函数，但使用 M 简化计算的方法不适用于逻辑回归，计算将非常耗时）</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211213536823.png" alt="image-20250211213536823" style="zoom:33%;" /><p><strong>（4）参数的选择</strong></p><p>1.参数C：当 C 较大，相当于$\lambda$小，可能会导致过拟合，高方差 variance;<br>当 C 较小，相当于 $\lambda$ 大，可能会导致欠拟合，高偏差 bias;</p><p>2.参数$\sigma^2$：当 $\sigma$ 较大时，图像扁平，可能会导致低方差，高偏差 bias;<br>当 $\sigma$ 较小时，图像窄尖，可能会导致低偏差，高方差 variance。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211214003886.png" alt="image-20250211214003886" style="zoom:33%;" /><h2 id="12-6-使用SVM"><a href="#12-6-使用SVM" class="headerlink" title="12.6 使用SVM"></a>12.6 使用SVM</h2><p><strong>（1）核函数选择</strong></p><p>通常使用现有的软件包来最小化 SVM 代价函数。在使用SVM时我们要明确1.参数C 2.核函数的选择</p><p>第一种核函数：No kernel（线性核函数linear kernel）。即不使用核函数。线性核函数 SVM 适用于函数简单，或特征n非常多而实例m非常少的情况，想要拟合一个线性决策边界的情况。$h_\theta(x)&#x3D;g(\theta_0+\theta_1x_1+…+\theta_nx_n)$, predict y&#x3D;1 if $\theta^Tx\ge0$.</p><p>第二种核函数：高斯核函数。适应于特征数n很小，样本数m很大，想要拟合出复杂非线性决策边界的情况。$h_\theta(x)&#x3D;g(\theta_0+\theta_1f_1+…+\theta_nf_n)$, 另外还需要选择参数$\sigma^2$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211215619788.png" alt="image-20250211215619788" style="zoom:33%;" /><p><strong>（2）特征缩放</strong></p><p>在使用高斯核函数之前要进行<strong>特征缩放</strong>。这样保障SVM可以关注所有不同的特征变量。而不是只被某一个数值特别大的特征影响。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211221609143.png" alt="image-20250211221609143" style="zoom:33%;" /><p><strong>（3）其他kernel</strong></p><p>除高斯核函数、线性核函数之外，还有其他一些选择，如：<br>多项式核函数(Polynomial Kernel)， 字符串核函数(String kernel)， 卡方核函数( chi-square kernel) ，直方图交集核函数(histogram intersection kernel) 等。<br>它们的目标也都是根据训练集和标识之间的距离来构建新特征。但其他核函数都使用很少。</p><p>一个核函数需要满足 Mercer’s 定理，才能被 SVM 的优化软件正确处理。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250211222445105.png" alt="image-20250211222445105" style="zoom: 33%;" /><p><strong>（4）多分类问题</strong></p><p>假设我们要分为K个类别，可以使用一对多的方法来解决多分类问题，训练K个SVM：对于第i个SVM，将$y&#x3D;i$作为正类，其余作为负类。这样可以得到K组参数$\theta$，选择具有最大$(\theta^{(i)})^Tx$的类别$i$，作为预测结果。</p><p>但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216151224888.png" alt="image-20250216151224888" style="zoom:33%;" /><p><strong>（5）逻辑回归模型 和 SVM 的选择</strong></p><p>可以根据以下准则进行模型的选择：$n$为特征数，$m$为训练样本数。<br>(a)如果$n&gt;&gt;m$，例如 $n$ 为10000，而 $m$ 在10-1000之间，即训练集数据量不够支持我们训练一个复杂的非线性模型，选用逻辑回归模型或者不带核函数的 SVM。<br>(b)如果 $n$ 较小，$m$ 中等，例如 $n$ 在 1-1000 之间，而 $m$ 在 10-10000 之间，使用高斯核函数的 SVM。<br>(c) 如果 $n$ 较小，$m$ 较大，例如 $n$ 在 1-1000 之间，而 $m$ 大于 50000，则使用 SVM 会非常慢。解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的 SVM。(如果训练集非常大，高斯核函数的 SVM 会非常慢。)</p><p>（注： 逻辑回归和不带核函数的SVM 非常相似。但是根据实际情况，其中一个可能会更有效。随着 SVM 的复杂度增加、特征数量相当大时，不带核函数的SVM 就会表现得相当突出。）</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250216152537795.png" alt="image-20250216152537795" style="zoom:33%;" /><p>神经网络在大多数情况下都是有效的，但通常训练较慢。一个非常好的 SVM 实现包可能会运行得比较快比神经网络快很多，而且它的代价函数是凸函数，不存在局部最优解。</p>]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>11.机器学习系统设计</title>
    <link href="/2025/02/16/11%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"/>
    <url>/2025/02/16/11%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="11-机器学习系统设计"><a href="#11-机器学习系统设计" class="headerlink" title="11 机器学习系统设计"></a>11 机器学习系统设计</h1><h2 id="11-1-确定执行的优先级"><a href="#11-1-确定执行的优先级" class="headerlink" title="11.1 确定执行的优先级"></a>11.1 确定执行的优先级</h2><p>本章使用垃圾邮件分类器的例子，来描述机器学习系统设计方法。</p><p>设计垃圾邮件分类器时，首先要确定特征向量$x$和$y$，这里我们选取100个单词组成我们的特征向量，当单词在邮件中出现，我们就将特征向量$x$对应位置出标为1.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250208200801676.png" alt="image-20250208200801676" style="zoom:33%;" /><p>接下来我们要考虑的就是如何提高分类器的准确率呢？</p><p>通常有很多方法：1.收集更多垃圾邮件和非垃圾邮件样本 2.基于邮件的路由信息设计更复杂的特征（考虑发件人信息、邮件标题）3.基于邮件正文信息设计更复杂的特征 4. 为探测刻意的拼写错误(把 watch 写成 w4tch)开发复杂的算法</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250208201329654.png" alt="image-20250208201329654" style="zoom:33%;" /><p>哪个才是最有效的呢？</p><h2 id="11-2-误差分析"><a href="#11-2-误差分析" class="headerlink" title="11.2 误差分析"></a>11.2 误差分析</h2><p>当设计一个机器学习系统时，1.通常先简单快速的实现算法，并在验证集上进行测试，2. 画出<strong>学习曲线</strong>，分析是否存在高偏差&#x2F;高方差问题，决定是否需要更多数据，还是更多特征。3. <strong>误差分析</strong>：人为检查分析验证集上出错的数据，看它们有哪些特征，算法有哪些优缺点。</p><p>2和3可以为我们的后续优化做指导</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250208202429552.png" alt="image-20250208202429552" style="zoom:33%;" /><p>通过以下方式分析统计验证集上犯错的样本：1. 邮件的类型 2. 没有被正确分类的原因。</p><p>通过数字体现出问题的大头，然后着重去解决问题的大头。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250208203447966.png" alt="image-20250208203447966" style="zoom:33%;" /><p>通常使用<strong>数值评价指标</strong>来判断一个方法是否对算法优化有效。比如：不使用单词主干提取在验证集的错误率是5%，使用单词主干提取在验证集的错误率是3%. 有效则采用。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250208205152035.png" alt="image-20250208205152035" style="zoom:33%;" /><p>总计：先简单粗暴实现算法，然后通<strong>过误差分析，分析出现了哪些失误，以此决定之后的优化方法。使用数值评价指标来试验新的想法，来确定这些想法哪些有用&#x2F;无用。</strong></p><h2 id="11-3-类偏斜的误差度量-Error-Metrics-for-Skewed-Classes"><a href="#11-3-类偏斜的误差度量-Error-Metrics-for-Skewed-Classes" class="headerlink" title="11.3 类偏斜的误差度量 Error Metrics for Skewed Classes"></a>11.3 类偏斜的误差度量 Error Metrics for Skewed Classes</h2><p>偏斜类：正负类别样本数量差距非常大。事实上只有0.5%得人得癌症，是否得癌症就是一个偏斜类。假设在癌症检测算法中，我们的算法错误率为1%，那么我们得算法还不如全输出$y&#x3D;0$的算法（错误率0.5%）</p><p>**在偏斜类问题上使用分类错误率或分类准确度不能恰当的评价算法的好坏。**对于偏斜数据集，如果单纯考虑准确率accuracy，会导致有时候模型预测的结果，还不如全部判断为1或者全部判断0 的结果好。 所以需要引入另外一些辅助度量指标。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250208212553201.png" alt="image-20250208212553201" style="zoom:33%;" /><p>这里我们让$y&#x3D;1$表示数量少的类别（得癌症）</p><p>查准率：在我们算法预测得癌症的人中，有多大比率的病人是真正患有癌症的。$precision&#x3D;\frac{正确预测患有癌症人数}{预测阳性}&#x3D;\frac{真阳性}{真阳性+假阳性}$</p><p>查全率（召回率）：数据集中确实得癌症的病人，有多少我们正确预测他们得了癌症。$recall&#x3D;\frac{正确预测患有癌症人数}{实际阳性}&#x3D;\frac{真阳性}{真阳性+假阴性}$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250208214504393.png" alt="image-20250208214504393" style="zoom:33%;" /><p><strong>查准率和查全率都是越高越好。在偏斜类问题中，使用查准率和召回率比使用准确率要好得多。</strong></p><h2 id="11-4-查准率Precision-和-查全率Recall-之间的权衡"><a href="#11-4-查准率Precision-和-查全率Recall-之间的权衡" class="headerlink" title="11.4 查准率Precision 和 查全率Recall 之间的权衡"></a>11.4 查准率Precision 和 查全率Recall 之间的权衡</h2><p>假设我们使用逻辑回归来预测癌症，输出$h_\theta(x)$介于[0, 1]之间。我们先设阈值&#x3D;0.5，若$h_\theta(x) \ge 0.5$，预测为正例，得癌症。若我们把阈值修改为0.7，那么我们预测得癌症的人有更大的可能性确实得癌症，判断的准、但有更多正例被漏掉，即有更高的查准率，但有更低的查全率。相反，若我们把阈值修改为0.3，那么我们找的全，但有更多负例被错判为正例，即有更高的查全率，但有更低的查准率。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250209195928045.png" alt="image-20250209195928045" style="zoom:33%;" /><p>那么我们如何权衡查准率和查全率，当不同算法或者不同阈值，有着不同的查准率和查全率，我们该怎么判断哪个算法更好呢？</p><p>一个结合了查准率和查全率的数值评价指标是F1分数。$F_1 score&#x3D;2\frac{PR}{P+R}$，只要P或R有一个低，F1分数就会很低。F1分数越高，算法表现越好。</p><p>如何自动选取阈值？可以使用不同的阈值在验证集上计算F1分数，分数最高的算法表现好。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250209201920414.png" alt="image-20250209201920414" style="zoom:33%;" /><h2 id="11-5-数据集的大小"><a href="#11-5-数据集的大小" class="headerlink" title="11.5 数据集的大小"></a>11.5 数据集的大小</h2><p>对于机器学习，通常可以选择很多不同的算法进行预测，随着训练集规模增大，准确率一般会提高：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250209205401138.png" alt="image-20250209205401138" style="zoom:33%;" /><p>但事实上，单纯增大数据集并不能解决一切问题。 如果数据集中含的信息很少（比如想对房价进行预测，但是只有面积数据。这时候即使增加数据、或者对模型在面积这个feature上进行多项式处理，也起不到好的效果）</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250209210245426.png" alt="image-20250209210245426" style="zoom:33%;" /><p>当我们的算法使用的特征值有足够的信息，即参数多，这将会是一个低偏差的算法。这时$J_{train}(\theta)$会很小；</p><p>当我们使用一个非常大的数据集时（不太可能过拟合），这将会是一个低方差的算法。这时$J_{train}(\theta) \approx J_{test}(\theta)$.</p><p>若我们结合两者，就能得到一个$J_{test}(\theta)$会很小的优秀算法。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250209204628658.png" alt="image-20250209204628658" style="zoom:33%;" /><p><strong>总结：如果我们能做到1.人类专家能够根据特征值$x$预测出来$y$（即，特征值有足够的信息）, 2.有一个庞大数据集。那么我们就能训练出性能好的学习算法。</strong></p><p><strong>如果模型欠拟合（偏差bias大），那么就要增加特征（对神经网络增加hidden units）；如果模型过拟合（方差variance大），那么就要增大数据集，使得$J_{cv} \approx J_{train}$，从而降低过拟合。</strong></p>]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>10.应用机器学习的建议</title>
    <link href="/2025/02/16/10%20%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE/"/>
    <url>/2025/02/16/10%20%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="10-应用机器学习的建议"><a href="#10-应用机器学习的建议" class="headerlink" title="10 应用机器学习的建议"></a>10 应用机器学习的建议</h1><h2 id="10-1-决定下步做什么"><a href="#10-1-决定下步做什么" class="headerlink" title="10.1 决定下步做什么"></a>10.1 决定下步做什么</h2><p>当我们想要改进一种算法的效果时，我们该做什么样的尝试？哪些是有意义的呢？</p><p>我们通常可以在以下几种方向进行尝试：1. 获取更多的训练数据 2. 尝试更少特征 3. 尝试更多特征 4. 尝试添加多项式特征 5. 减小$\lambda$ 6. 增大$\lambda$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250206195003818.png" alt="image-20250206195003818" style="zoom:50%;" /><p>接下来将介绍一些诊断法，让我们清楚如何选择更有效的方法。节省时间</p><p>机器学习诊断法的定义：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250206195351622.png" alt="image-20250206195351622" style="zoom:50%;" /><h2 id="10-2-评估一个假设"><a href="#10-2-评估一个假设" class="headerlink" title="10.2 评估一个假设"></a>10.2 评估一个假设</h2><p>当代价函数很小时，不一定这个假设就是好的，可能是过拟合。</p><p>那么我们<strong>如何判断是否发生了过拟合呢？<strong>当特征数较少时，我们可以画图来观察。当特征数较多时，我们可以</strong>划分出训练集和测试集</strong></p><p>当数据不是随机分布的，我们最好先打乱，或者随机选择70%数据作为训练集，剩下的30%作为测试集。当数据是随机分布的，可以选择前70%数据作为训练集，剩下的30%作为测试集。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250206200353378.png" alt="image-20250206200353378" style="zoom:33%;" /><p>训练的过程大致如下：</p><p>1.对于回归问题。例如线性回归。先使用训练集进行训练（最小化代价函数，学习参数$\theta$），再使用测试集计算测试误差：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250206201409259.png" alt="image-20250206201409259" style="zoom:33%;" /><p>2.对于分类问题。例如逻辑回归。过程同上，只是代价函数不同：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250206201511718.png" alt="image-20250206201511718" style="zoom:33%;" /><p>还有一种简单的测试误差，叫做错分率（0&#x2F;1错分率）：分类预测结果$h_\theta$(x)错误，则$err$值为1；如果预测正确，则$err$值为0. 整体的测试误差为所有$err$的均值.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250206201634983.png" alt="image-20250206201634983" style="zoom:33%;" /><h2 id="10-3-模型选择-和-训练-验证-测试集"><a href="#10-3-模型选择-和-训练-验证-测试集" class="headerlink" title="10.3 模型选择 和 训练&#x2F;验证&#x2F;测试集"></a>10.3 模型选择 和 训练&#x2F;验证&#x2F;测试集</h2><p>产生过拟合的一个原因是：我们在训练集上训练得到参数$\theta$，得到的训练误差，通常不能作为对实际泛化误差的一个好的估测。</p><p>当需要确定最合适的多项式次数，那么怎样选用正确的特征来构造学习算法；或者当需要选择学习算法中正则化参数$\lambda$，这类问题被叫做模型选择问题。</p><p>假设针对$x$有10个模型：一次多项式到十次多项式。对于每个多项式，在训练集上训练出$\theta$。然后在测试集上计算误差，选择最小的$J_{test}(\theta^{(i)})$. 因此选择d&#x3D;5这个模型。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250206204714973.png" alt="image-20250206204714973" style="zoom:33%;" /><p>但这里有个问题：我们选的这个模型，就是能够最好地拟合测试集的参数d的值及多项式的度。因此，再使用同样的测试集来评价假设，显然很不公平，很可能导致过拟合。说白了由于模型选择的加入，原来的测试集也被用来拟合参数而非真正的测试假设函数的误差。</p><p>所以，我们将数据集分为6:2:2三部分，分别为训练集、验证集、测试集。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250206204753411.png" alt="image-20250206204753411" style="zoom:33%;" /><p>同样我们可以得到在每个集合上误差计算公式：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250206204814550.png" alt="image-20250206204814550" style="zoom:33%;" /><p>现在我们是用 验证集计算误差，分别得到$ J_{cv}(θ^{(1)}),…J_{cv}(θ^{(10)})$，发现 $J_{cv}(θ^{(4)}) $的值最小，因此选择 d&#x3D;4 这个模型，最后在测试集上进行预测，能得到一个更理想的泛化误差。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250206211043912.png" alt="image-20250206211043912" style="zoom:33%;" /><p>总结一句话，测试集不能参与调参（不能参与选最终模型）就完了。</p><h2 id="10-4-诊断偏差与方差"><a href="#10-4-诊断偏差与方差" class="headerlink" title="10.4 诊断偏差与方差"></a>10.4 诊断偏差与方差</h2><p>若我们的学习算法表现不好，无非是偏差过大或者是方差过大。即欠拟合或过拟合。弄清楚是什么问题有助于我们来改进学习算法。</p><p>偏差bias大，为欠拟合underfitting；方差variance大，为过拟合overfitting.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250207195754961.png" alt="image-20250207195754961" style="zoom:50%;" /><p>当我们使用不同次数的多项式去拟合的时候，根据训练误差和验证误差的定义，我们可以画出训练误差、验证误差随多项式次数$d$的变化曲线。随着$d$的增大，会更好地拟合训练数据，$J_{train}(\theta)$不断下降。当$d$很大时，可能出现过拟合，$J_{cv}(\theta)$也很大；当$d$在某个合适值的时候，$J_{cv}(\theta)$值最小。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250207193909039.png" alt="image-20250207193909039" style="zoom:33%;" /><p><strong>如何区分是高偏差（欠拟合）or 高方差（过拟合）？</strong></p><p>曲线左侧区域为高偏差（欠拟合），$J_{train}(\theta)$很高，$J_{cv}(\theta)\approx J_{train}(\theta)$. 曲线右侧区域为高方差（过拟合），$J_{train}(\theta)$很小，$J_{cv}(\theta)&gt;&gt; J_{train}(\theta)$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250207195054796.png" alt="image-20250207195054796" style="zoom:33%;" /><h2 id="10-5-正则化-和-偏差-方差"><a href="#10-5-正则化-和-偏差-方差" class="headerlink" title="10.5 正则化 和 偏差&#x2F;方差"></a>10.5 正则化 和 偏差&#x2F;方差</h2><p>接下来讨论正则化项对偏差&#x2F;方差的影响。</p><p>考虑正则化的线性回归模型。当我们要拟合一个多项式时，若$\lambda$的值很大，则参数$\theta$会被惩罚很重，使$\theta \approx 0$, $h_\theta(x)&#x3D;\theta_0$变为一条直线，高偏差（欠拟合）。若$\lambda$的值很小，正则项不起作用，会得到高方差（过拟合）。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250207201307104.png" alt="image-20250207201307104" style="zoom: 33%;" /><p>那么<strong>如何选择出合适的正则化参数$\lambda$呢？</strong></p><p>这里我们定义在训练集、验证集、测试集上的损失函数时，不考虑$\lambda$（为了比较$\lambda$对$\theta$的影响）。训练时用的是$J$（包含正则化项）来求$\theta$，而$J_{train}$和$J_{cv}$只是用来画线说明问题.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250207204707302.png" alt="image-20250207204707302" style="zoom: 50%;" /><p>我们可以按照步长两倍的方式递增$\lambda$，针对每个$\lambda$最小化代价函数$J(\theta)$, 训练出参数向量$\theta$。然后分别计算对应的$J_{cv}(\theta)$，得到最小的$J_{cv}(\theta^{(5)})$。然后在 test 集合上进行测试。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250207202658411.png" alt="image-20250207202658411" style="zoom:33%;" /><p>我们画出$J_{train}(\theta)、J_{cv}(\theta)$随$\lambda$的变化曲线。当$\lambda$小时，正则化程度弱，很容易出现高次多项式，$J_{train}(\theta)$会很小；当$\lambda$大时，正则化程度强，出现欠拟合，$J_{train}(\theta)$会很大。当$\lambda$小时，很容易出现过拟合，$J_{cv}(\theta)$会很大；当$\lambda$大时，正则化程度强，出现欠拟合，$J_{cv}(\theta)$会很大。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250207203850169.png" alt="image-20250207203850169" style="zoom:33%;" /><h2 id="10-6-学习曲线"><a href="#10-6-学习曲线" class="headerlink" title="10.6 学习曲线"></a>10.6 学习曲线</h2><p>学习曲线可以用于检查学习算法运作是否正常、改进算法表现</p><p>m表示训练样本的个数。1.训练误差：当m&#x3D;3时，人为减少训练数据个数，只用假设拟合这三个训练样本，计算训练误差。当训练样本很少的时候，很容易把训练集拟合到很好。当m增大时，想把每个训练样本都拟合很好会变得困难，所以训练误差会增大。2.验证误差：数据越多，越能获得更好地泛化表现，越能拟合出合适的假设。所以验证误差会下降。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250207212943967.png" alt="image-20250207212943967" style="zoom:50%;" /><p>我们来看**高偏差（欠拟合）**的情况：假设使用一次函数来进行拟合。1.训练集误差：当m小时，很容易拟合的很好，$J_{train}(\theta)$很小；当m大时，欠拟合，更难把训练数据拟合很好了，$J_{train}(\theta)$会变大。2.验证集误差：m小时，表现当然不好；随着m增大到某值，几乎拟合出了最佳直线，即使继续增大m，还是会得到一条差不多的直线，此时$J_{cv}(\theta)$曲线几乎水平不再变化。最后，两条曲线会非常接近，因为当参数很少又有很多数据时，训练误差和验证误差会很接近。</p><p>观察曲线得知：高偏差情况下$J_{train}(\theta)、J_{cv}(\theta)$都很高，并且更多的训练样本并不能改善。高偏差情形：模型过于简单，数据再多也于事无补。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250207211143411.png" alt="image-20250207211143411" style="zoom:50%;" /><p>再来看**高方差（过拟合）**的情况：假设使用100次函数来进行拟合。1.训练集误差：当m小时，很容易拟合的很好，$J_{train}(\theta)$很小；当m大时，更难把训练数据拟合很好了，$J_{train}(\theta)$会变大。2.验证集误差：在高方差情况下假设函数对数据过拟合，因此验证集误差会一直很大。</p><p>观察曲线得知：高方差情况下$J_{train}(\theta)&lt;J_{cv}(\theta)$，并且有一定的差距。这时更多的训练样本是能够改善的。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250207212338888.png" alt="image-20250207212338888" style="zoom:33%;" /><h2 id="10-7-接下来做什么"><a href="#10-7-接下来做什么" class="headerlink" title="10.7 接下来做什么"></a>10.7 接下来做什么</h2><p>每种解决方案对应的问题如下:</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250207215928026.png" alt="image-20250207215928026" style="zoom:33%;" /><p>小的神经网络，计算量小，但容易欠拟合。大型神经网络。计算量大，可以用正则化项来修正过拟合。如果不知道选择几层hidden layer，可以将数据分为训练集、验证集、测试集之后，比较验证损失$J_{cv}(\theta)$来决定。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250207220031335.png" alt="image-20250207220031335" style="zoom:50%;" />]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>9.神经网络学习</title>
    <link href="/2025/02/16/9%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/"/>
    <url>/2025/02/16/9%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="9-神经网络学习"><a href="#9-神经网络学习" class="headerlink" title="9 神经网络学习"></a>9 神经网络学习</h1><h2 id="9-1-代价函数"><a href="#9-1-代价函数" class="headerlink" title="9.1 代价函数"></a>9.1 代价函数</h2><p>分类问题：二元分类、多分类</p><p>我们用L表示总层数，$s_l$表示第$l$层的神经元个数，常用$s_l$或$K$表示最后一层神经元的个数。</p><p>在二元分类中，输出$y&#x3D;0 or 1\in\mathbb{R}$，$s_l&#x3D;K&#x3D;1$.在多元分类中，输出$y\in\mathbb{R}^K$, $s_l&#x3D;K$, $(K \ge3)$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250113140327471.png" alt="image-20250113140327471" style="zoom:50%;" /><p>神经网络中我们的代价函数与逻辑回归中的代价函数类似，我们的输出$h_\Theta(x)\in\mathbb{R}^K$,其中$(h_\Theta(x))_i$表示输出向量中第i项的值。</p><p>首先前面的项多了$\sum_{k&#x3D;1}^{K}$表示将输出层<strong>每个</strong>神经元的预测值与真实值之间的距离加和，后面项是规格化项，这里通常规定不计算每一层的$\theta_0$，然后将每一层的$\theta$矩阵平方加和。即所有参数的平方和。（计算$\theta_0$也是可以的，但通常不这样做）</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250113165449147.png" alt="image-20250113165449147" style="zoom:50%;" /><h2 id="9-2-反向传播算法"><a href="#9-2-反向传播算法" class="headerlink" title="9.2 反向传播算法"></a>9.2 反向传播算法</h2><p>为了执行梯度下降算法，我们需要计算$J(\Theta)$和$J(\Theta)$对各个参数$\Theta_{ij}^{(l)}$的偏导。</p><p>假设我们的网络如右图，只有一个样本$(x,y)$时我们先进行前向传播，计算出以下：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250113171928633.png" alt="image-20250113171928633" style="zoom:50%;" /><p>定义$\delta_j^{(l)}$表示第l层第j个节点的误差。则$\delta_j^{(4)}&#x3D;(h_\Theta(x))_j-y_j&#x3D;a_j^{(4)}-y_j$。我们也可以写成向量的形式$\delta^{(4)}&#x3D;a^{(4)}-y，\delta^{(4)},a^{(4)},y\in\mathbb{R}^K$，第3层的误差项为$\delta^{(3)}&#x3D;(\Theta^{(3)})^T\delta^{(4)}.<em>g’(z^{(3)})$,其中$g’(z^{(3)})$是sigmoid函数的导数，经推导sigmoid函数导数有一个特点：$g’(z^{(3)})&#x3D;a^{(3)}.</em>(1-a^{(3)})$。</p><p>同理$\delta^{(2)}&#x3D;(\Theta^{(2)})^T\delta^{(3)}.*g’(z^{(2)})$, $g’(z^{(2)})$类似.注意第一层没有误差项，因为我们不想改变我们的输入$x$.</p><p>当我们忽略正则化项，即当$\lambda&#x3D;0$时。经推导，$J(\Theta)$对各矩阵参数$\Theta_{ij}^{(l)}$的偏导为$a_j^{(l)}\delta_i^{(l+1)}$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250113171510938.png" alt="image-20250113171510938" style="zoom: 33%;" /><p>下面是反向传播算法的伪代码。当有m个样本时，变量i遍历每个样本，令$a^{(1)}&#x3D;x^{(i)}$,然后执行前向传播计算出$a^{(l)}$,再使用$y^{(i)}$计算出$\delta^{(L)}$，然后进行反向传播计算出$\delta^{(L-1)},\delta^{(L-2)}…\delta^{(2)}$. 使用$\Delta_{ij}^{(l)}$累加各偏导数项$a_j^{(l)}\delta_i^{(l+1)}$，写成向量的形式即为$\Delta^{(l)}:&#x3D;\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250114192316746.png" alt="image-20250114192316746" style="zoom: 80%;" /><p>当$j&#x3D;0$时没有偏差项。计算代价函数的偏导数，公式如下：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250114194028905.png" alt="image-20250114194028905" style="zoom:80%;" /><h2 id="9-3-理解反向传播"><a href="#9-3-理解反向传播" class="headerlink" title="9.3 理解反向传播"></a>9.3 理解反向传播</h2><p>当只有一个输出单元时，前向传播的过程：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250114201012101.png" alt="image-20250114201012101" style="zoom:80%;" /><p>当只考虑一个样本i时。我们简化代价函数，去掉正则化项，得到 cost(i)：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250114201230325.png" alt="image-20250114201230325" style="zoom:80%;" /><p>反向传播的过程：$\delta^{(4)}_1&#x3D;y^{(i)}-a^{(i)}<em>1$。$\delta^{(3)}<em>2&#x3D;\Theta</em>{12}^{(3)}\delta_1^{(4)},\delta^{(2)}<em>2&#x3D;\Theta</em>{12}^{(2)}\delta_1^{(3)}+\Theta</em>{22}^{(2)}\delta_2^{(3)}$.</p><p>即对于每一层来说，$\delta$分量都等于后面一层所有的$\delta$的加权和，其中权值就是参数$\Theta$:$\delta_j^{(l)}&#x3D;\sum_{k&#x3D;1}^{s_{(l+1)}-1}\Theta_{kj}^{(l)}\delta_k^{(l+1)}$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250114195858686.png" alt="image-20250114195858686" style="zoom:80%;" /><h2 id="9-4-展开参数"><a href="#9-4-展开参数" class="headerlink" title="9.4 展开参数"></a>9.4 展开参数</h2><p>使用矩阵表达式的好处：更方便进行前向和反向传播；使用向量表达式的好处：优化算法通常需要将参数展开成长向量的形式。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250205193027431.png" alt="image-20250205193027431" style="zoom:50%;" /><h2 id="9-5-梯度检验"><a href="#9-5-梯度检验" class="headerlink" title="9.5 梯度检验"></a>9.5 梯度检验</h2><p>在反向传播中，因为其复杂性通常会产生一些bug，虽然代价函数$J(\theta)$也在下降，但最终的代价可能比在没有bug的情况下高出一个量级，通常使用梯度检验来解决这个问题。</p><p>我们可以在$\theta$点两侧加减$\epsilon$的距离，根据斜率估计出$\theta$点的导数。$\epsilon$取很小的值。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250205194000939.png" alt="image-20250205194000939" style="zoom: 33%;" /><p>当有多个参数时，例如当$\theta \in \mathbb{R}^n$时，长度为n的向量。我们可以分别在各参数上加减$\epsilon$，来计算代价函数对各参数的偏导数。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250205194407308.png" alt="image-20250205194407308" style="zoom: 33%;" /><p>算法实现如下：对每个参数分别计算偏导得到gradApprox，再与反向传播计算得到的偏导Dvec进行比较，当两者较为接近时，说明反向传播的实现是正确的。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250205195155303.png" alt="image-20250205195155303" style="zoom:33%;" /><p>应用步骤：1.先反向传播计算DVec 2.数值估计计算gradApprox 3.确保两者数值差距较小，否则说明反向传播算法有错误。</p><p>梯度检验计算量大，准确性高；反向传播计算速度更快，但实现复杂可能出现问题。所以当我们确定下来反向传播没问题时，正式训练之前需要关闭梯度检验，否则训练过程会很慢。（只在测试时进行梯度检验）</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250205195717287.png" alt="image-20250205195717287" style="zoom:33%;" /><h2 id="9-6-随机初始化"><a href="#9-6-随机初始化" class="headerlink" title="9.6 随机初始化"></a>9.6 随机初始化</h2><p>在之前的逻辑回归中，我们将参数$\theta$全部初始化为0。在神经网络中，此方法不可行：如果第一层参数$\theta$都相同(不管是不是0)，意味着第二层的所有激活单元的值会完全相同。因为初始权重相等，权重更新值也相等。这意味着最后的逻辑回归单元只能得到一个特征。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250205201534850.png" alt="image-20250205201534850" style="zoom:33%;" /><p>为了解决这个问题，在神经网络中通常将参数初始化为$[-\epsilon,\epsilon]$之间的随机值。参考代码如下：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250205202431990.png" alt="image-20250205202431990" style="zoom:33%;" /><h2 id="9-7-综合起来"><a href="#9-7-综合起来" class="headerlink" title="9.7 综合起来"></a>9.7 综合起来</h2><p>神经网络训练的步骤如下：</p><p><strong>0.选择网络结构</strong>：确定输入单元数、输出单元数、隐藏层个数、每个隐藏层的单元数。</p><p>输入单元数通常等于特征向量$x$的维度。输出单元数通常等于待分类的类别数。隐藏层数：比较合理的是1层隐藏层，或者多于1层且每层的单元数相等。隐藏层的单元数通常等于输入单元数或大几倍。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250205203213254.png" alt="image-20250205203213254" style="zoom:33%;" /><p><strong>1.随机初始化参数</strong></p><p><strong>2.应用前向传播计算出</strong>$h_\theta(x^{(i)})$</p><p><strong>3.计算代价函数$J(\Theta)$</strong></p><p><strong>4.应用反向传播计算偏导数</strong></p><p><strong>5.利用梯度检测方法检验这些偏导数</strong>：检验通过后关闭</p><p><strong>6.使用优化算法来最小代价函数</strong>：反向传播计算出偏导数，使用优化算法最小化代价函数$J(\Theta)$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250205204122375.png" alt="image-20250205204122375" style="zoom:33%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250205204904433.png" alt="image-20250205204904433" style="zoom:33%;" /><p>代价函数不是一个凸函数，我们使用优化算法通常可以到达一个局部最小值点。</p><p>反向传播算法计算出梯度下降的方向，优化算法让代价函数沿着这个方向一点点的下降。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20250205205656384.png" alt="image-20250205205656384" style="zoom:33%;" />]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>8.神经网络学习</title>
    <link href="/2025/02/16/8%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/"/>
    <url>/2025/02/16/8%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="8-神经网络学习"><a href="#8-神经网络学习" class="headerlink" title="8 神经网络学习"></a>8 神经网络学习</h1><h2 id="8-1-非线性假设"><a href="#8-1-非线性假设" class="headerlink" title="8.1 非线性假设"></a>8.1 非线性假设</h2><p>当特征数量n很大时，使用线性回归或逻辑回归进行拟合会让特征空间急剧膨胀，复杂度高且容易过拟合。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225085717427.png" alt="image-20241225085717427" style="zoom:50%;" /><p>eg当输入是50*50像素的灰度图像时，$n&#x3D;50\times50&#x3D;2500$假设我们只考虑二次特征，那么二次特征($x_i\times x_j$)的数量为$\frac{n^2}{2}$,计算量大。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225091144996.png" alt="image-20241225091144996" style="zoom:50%;" /><h2 id="8-2-模型表示"><a href="#8-2-模型表示" class="headerlink" title="8.2 模型表示"></a>8.2 模型表示</h2><p>我们定义一个简单的逻辑单元，其中$x_1, x_2, x_3$作为输入，$x_0$称为偏差（$x_0$恒为1），$h_\theta(x)&#x3D;\frac{1}{1+e^{-\theta^Tx}}$是输出，sigmoid$g(z)&#x3D;\frac{1}{1+e^{-z}}$是激活函数.$\theta$称为参数或权重。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225093823220.png" alt="image-20241225093823220" style="zoom:50%;" /><p>当有多个逻辑单元（神经元）连在一起的时候，形成神经网络。其中第一层称为输入层（input layer），最后一层称为输出层（output layer），其余层被称为隐藏层（hidden layer）</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225094829711.png" alt="image-20241225094829711" style="zoom:50%;" /><p>$a_i^{(j)}$表示第j层第i个神经元的输出，$\Theta^{(j)}$表示从第j层到第j+1层映射的权重矩阵。$\Theta_{ba}^{(j)}$其实就是表示从第j层的第a个单元到第j+1层第b个单元的权重</p><p>若第j层有$s_j$个神经元，第j+1层有$s_{j+1}$个神经元，那么参数矩阵$\Theta^{(j)}$的维度是$s_{j+1}\times(s_j+1)$。其中+1是因为第j层有偏差神经元默认隐藏了。eg$\Theta^{(1)}\in \mathbb{R}^{3\times4}$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225111757980.png" alt="image-20241225111757980" style="zoom:50%;" /><p>前向传播与向量实现：为了统一表示方法，我们将输入$x$表示为第1层的激活项$a^{(1)}&#x3D;[x_0;x_1;x_2;x_3]$。记$z^{(2)}&#x3D;\Theta^{(1)}a^{(1)}$, 第2层的激活项为$a^{(2)}&#x3D;g(z^{(2)})\in \mathbb{R}^3$, 我们添加$a^{(2)}<em>0$项，此时$a^{(2)}\in \mathbb{R}^4$. 重复以上过程，得到$z^{(3)}&#x3D;\Theta^{(2)}a^{(2)}, h</em>\Theta(x)&#x3D;a^{(3)}&#x3D;g(z^{(3)})$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225144138268.png" alt="image-20241225144138268" style="zoom:50%;" /><p>若遮住第1层，我们发现模型剩余部分就是逻辑回归，只是输入不再是$x_1,x_2,x_3$，而是$a^{(2)}_1,a^{(2)}_2,a^{(2)}_3$.这是从输入值学习得到的特征项。即<strong>神经网络并没有用输入特征$x_1,x_2,x_3$来训练逻辑回归，而是自己训练逻辑回归的输入$a_1,a_2,a_3$(复杂特征项)</strong></p><p>我们也可以使用多项式项作为输入如$x_1x_2,x_2x_3$等，神经网络会灵活的尝试快速学习任意的特征项。体现在参数$\Theta$上。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225150431778.png" alt="image-20241225150431778" style="zoom:50%;" /><h2 id="8-3-例子与直观理解"><a href="#8-3-例子与直观理解" class="headerlink" title="8.3 例子与直观理解"></a>8.3 例子与直观理解</h2><p>改变权重就能实现不同的逻辑</p><p>与运算：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225162516005.png" alt="image-20241225162516005" style="zoom:50%;" /><p>或运算：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225162639762.png" alt="image-20241225162639762" style="zoom: 50%;" /><p>非运算：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225164429955.png" alt="image-20241225164429955" style="zoom:50%;" /><p>同或运算：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225164546834.png" alt="image-20241225164546834" style="zoom:50%;" /><p>通过不断用更深的层，可以计算更复杂的函数。每层都在上一层的基础上计算更复杂的方程。</p><h2 id="8-4-多元分类"><a href="#8-4-多元分类" class="headerlink" title="8.4 多元分类"></a>8.4 多元分类</h2><p>当有4个类别需要进行分类时，我们让神经网络的输出层拥有4个神经元，即输出$h_\Theta(x)\in \mathbb{R}^4$是4维向量。第i个神经元的输出表示是否为第i类。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225170428125.png" alt="image-20241225170428125" style="zoom:50%;" /><p>我们训练集中一个样本表示为$(x^{(i)},y^{(i)})$, 其中$x^{(i)}$是一张图像，$y^{(i)}$是独热编码，我们要训练模型$h_\Theta(x^{(i)})$，让它尽可能的接近$y^{(i)}$。其中$h_\Theta(x^{(i)})$和$y^{(i)}$都是4维的向量。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241225170758496.png" alt="image-20241225170758496" style="zoom:50%;" />]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>6.正则化</title>
    <link href="/2025/02/16/6%20%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <url>/2025/02/16/6%20%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="6-正则化"><a href="#6-正则化" class="headerlink" title="6 正则化"></a>6 正则化</h1><h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><p>当我们选择了过多的特征时，假设函数可能会拟合数据集非常好($J(\theta)&#x3D;\frac{1}{2m}\sum_{i&#x3D;1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2\approx0$), 但对新数据的泛化能力很差。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241221161252583.png" alt="image-20241221161252583" style="zoom:50%;" /><p>左图：欠拟合 高偏差(high bias)      右图：过拟合 高方差(high variance)</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241221161525443.png" alt="image-20241221161525443" style="zoom:50%;" /><p>我们可以绘制数据点的分布，来决定假设函数合适的多项式阶次。</p><p>如何解决过拟合：</p><p>（1）减少特征的数量（手动选择保留哪些特征、模型选择算法（自动选择保留哪些特征））</p><p>（2）正则化（保留所有特征，但减少量级或参数$\theta_j$的大小）当各特征都对预测y提供作用时，该方法是有用的。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241221162001230.png" alt="image-20241221162001230" style="zoom:50%;" /><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>假设我们修改代价函数，惩罚$\theta_3$和$\theta_4$，让这俩参数尽量小，那么假设函数就会更简单、更平滑。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241221164426548.png" alt="image-20241221164426548" style="zoom:50%;" /><p>当参数更小时，假设函数就会更简单、更平滑、更不容易出现过拟合。</p><p>由于我们事先不知道该减少哪些参数的大小，我们将正则化项加入到代价函数中，让所有参数$\theta$尽量小，减少过拟合。</p><p>注意：正则化项一般不对$\theta_0$进行惩罚。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241221164627601.png" alt="image-20241221164627601" style="zoom:50%;" /><p>$\lambda$是一个正则化参数，控制两个目标（更好地拟合目标or保持参数尽量小）的取舍。$\lambda$太大欠拟合，太小过拟合</p><p>$\lambda$太大时，$\theta_1,\theta_2,\theta_3,\theta_4\approx0$, $h_\theta(x)&#x3D;\theta_0$成为一条直线，发生欠拟合。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241221165454444.png" alt="image-20241221165454444" style="zoom:50%;" /><h2 id="正则化的线性回归模型"><a href="#正则化的线性回归模型" class="headerlink" title="正则化的线性回归模型"></a>正则化的线性回归模型</h2><p>线性回归的代价函数为$J(\theta)&#x3D;\frac{1}{2m}\sum_{i&#x3D;1}^m[((h_\theta(x^{(i)})-y^{(i)})^2]$</p><p>添加正则化项后：$J(\theta)&#x3D;\frac{1}{2m}\sum_{i&#x3D;1}^m[((h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j&#x3D;1}^{n}\theta_j^2]$</p><p>我们的代价函数如下，我们需要找到合适的$\theta$来最小化代价函数。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241221201123255.png" alt="image-20241221201123255" style="zoom:50%;" /><p>线性回归可以用梯度下降和正规方程来找到最佳的参数$\theta$</p><p><strong>梯度下降</strong>方法中，参数$\theta_0$的更新方程与之前一样（因为正则化项未对$\theta_0$进行惩罚）；其余参数的更新方程如下。我们将关于$\theta_j$的项整理在一起，其系数为$1-\alpha\frac{\lambda}{m}&lt;1$, 后面项与没有正则化方法的更新一致。所以正则化梯度下降方法，每次更新将参数$\theta_j$慢慢变小，慢慢向0靠近，同时参数更新方法未改变。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241221201231079.png" alt="image-20241221201231079" style="zoom:50%;" /><p><strong>正规方程</strong>方法中，矩阵$X$的第i行由第i个样本的特征向量组成。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241221205121828.png" alt="image-20241221205121828" style="zoom:50%;" /><p>我们去最小化$J(\theta)$，我们令$J(\theta)$对各参数$\theta$的偏导等于0，得到$\theta&#x3D;(X^TX+\lambda L)^{-1}X^Ty$, 其中<br>$$<br>L&#x3D;\left[<br>\begin{matrix}<br> 0      &amp;        &amp;        &amp;        \<br>        &amp; 1      &amp;        &amp;        \<br>        &amp;        &amp; \ddots &amp;        \<br>        &amp;        &amp;        &amp; 1      \<br>\end{matrix}<br>\right]<br>$$<br>当样本个数$m\le n$特征数量，$X^TX$是不可逆的non-invertible，但当$\lambda&gt;0$时，$X^TX+\lambda L$一定是可逆的invertible</p><h2 id="正则化的逻辑回归模型"><a href="#正则化的逻辑回归模型" class="headerlink" title="正则化的逻辑回归模型"></a>正则化的逻辑回归模型</h2><p>逻辑回归的代价函数为$J(\theta)&#x3D;-\frac{1}{m}\sum_{i&#x3D;1}^m[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$</p><p>添加正则化项后：$J(\theta)&#x3D;-\frac{1}{m}\sum_{i&#x3D;1}^m[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j&#x3D;1}^n\theta_j^2$</p><p>注：这个代价函数看上去同正则化线性回归的式子一样，但是两个$h$不同，所以有很大差别。$\theta_0$不参与任何正则化</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241221212723160.png" alt="image-20241221212723160" style="zoom:50%;" /><p>逻辑回归可以用梯度下降和高级算法来找到最佳的参数$\theta$</p><p><strong>梯度下降</strong>算法中，由于$\theta_0$不参与正则化，参数$\theta_0$的更新公式不变；其余参数的更新公式如下。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241221212802561.png" alt="image-20241221212802561" style="zoom:50%;" /><p><strong>高级算法</strong>中，我们需要自己实现计算$J(\theta)$ 和 $J(\theta)$对各个参数的偏导。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241221213819716.png" alt="image-20241221213819716" style="zoom:50%;" /><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">costReg</span>(<span class="hljs-params">theta, X, y, learningRate</span>):<br>    theta = np.matrix(theta)<br>    X = np.matrix(X)<br>    y = np.matrix(y)<br>    first = np.multiply(-y, np.log(sigmoid(X*theta.T)))<br>    second = np.multiply((<span class="hljs-number">1</span> - y), np.log(<span class="hljs-number">1</span> - sigmoid(X*theta.T)))<br>    reg = (learningRate / (<span class="hljs-number">2</span> * <span class="hljs-built_in">len</span>(X))* np.<span class="hljs-built_in">sum</span>(np.power(theta[:,<span class="hljs-number">1</span>:the<br>ta.shape[<span class="hljs-number">1</span>]],<span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(first - second) / (<span class="hljs-built_in">len</span>(X)) + reg<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5.Logistic回归</title>
    <link href="/2025/02/16/5%20Logistic%E5%9B%9E%E5%BD%92/"/>
    <url>/2025/02/16/5%20Logistic%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h1 id="5-Logistic回归"><a href="#5-Logistic回归" class="headerlink" title="5 Logistic回归"></a>5 Logistic回归</h1><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>在二分类问题中，样本的标签通常是$y\in {0,1}$,其中0表示负类，1表示正类</p><p>当我们对分类问题使用线性回归时，这并不是一种好方法。当我们把阈值设为0.5时，即当$h_{\theta}(x)\ge0.5时，输出y&#x3D;1$表示正类；当$h_{\theta}(x)&lt;0.5时，输出y&#x3D;0$表示负类。我们拟合的函数可能是图中蓝色线条，会将部分标签为1的预测成0。结果性能很差劲。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220142923076.png" alt="image-20241220142923076" style="zoom:50%;" /><p>在二分类问题中，标签是0或1。但线性回归的输出$h_{\theta}(x)可能大于1或小于0$.在logistic回归中，$0\le h_{\theta}(x)\le1$。虽然名字叫做回归，但这是一种用于分类的算法。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220143510020.png" alt="image-20241220143510020" style="zoom:50%;" /><h2 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h2><p>在线性回归中，我们的假设函数是$h_\theta(x)&#x3D;\theta^Tx$,为了让输出介于0和1之间，我们可以在$\theta^Tx$上套一个g(z),这里$g(z)&#x3D;\frac{1}{1+e^{-z}}$, 这里的g(z)称为sigmoid函数或logistic函数；所以我们的假设函数为$h_\theta(x)&#x3D;\frac{1}{1+e^{-\theta^Tx}}$，接下来就是寻找合适的$\theta$去拟合数据</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220145135831.png" alt="image-20241220145135831" style="zoom:50%;" /><p>我们的输出$h_\theta(x)$解释为在特征向量为$x$的前提下，y&#x3D;1(为正类)的可能性。eg，当$h_\theta(x)&#x3D;0.7$时，表示该病人有0.7的可能性是恶性肿瘤。即我们的输出$h_\theta(x)&#x3D;P(y&#x3D;1|x;\theta)$.因为为二分类问题，所以$P(y&#x3D;0|x;\theta)+P(y&#x3D;1|x;\theta)&#x3D;1$, 可得$P(y&#x3D;0|x;\theta)&#x3D;1-P(y&#x3D;1|x;\theta).$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220145928828.png" alt="image-20241220145928828" style="zoom:50%;" /><h2 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h2><p>当模型的预测为1的时候（或者说预测为1的可能性更大的时候$h_\theta(x)\ge0.5$）,观察$g(z)$的图像可知，即当$\theta^Tx\ge0$时，预测为1；当$\theta^Tx&lt;0$时，预测为0</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220152306160.png" alt="image-20241220152306160" style="zoom:50%;" /><p>假设我们的参数$\theta$为[-3; 1; 1]。预测为1时，即当$\theta^Tx\ge0$时（$-3+x_1+x_2\ge0$），为图像中的右上半区域；同理左下半区域预测为0.</p><p>我们称$-3+x_1+x_2&#x3D;0$为决策边界。当参数$\theta$确定后，决策边界就确定了下来，不依赖于数据集。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220152749975.png" alt="image-20241220152749975" style="zoom:50%;" /><p>我们可以添加额外的高阶多项式项，来拟合更复杂的决策边界。</p><p>在图一中，假设参数$\theta&#x3D;[-1;0;0;1;1]$，决策边界即为$x_1^2+x_2^2&#x3D;1$，$x_1^2+x_2^2\ge1$时的区域决策为1.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220154258647.png" alt="image-20241220154258647" style="zoom: 50%;" /><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>在logistic回归中，我们的假设函数为$h_\theta(x)&#x3D;\frac{1}{1+e^{-\theta^Tx}}$, 如何去拟合参数$\theta$呢</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220155912266.png" alt="image-20241220155912266" style="zoom:50%;" /><p>若我们直接使用线性回归中的代价函数，由于假设函数是非线性的，代价函数可能是非凸函数（会有局部最小值），梯度下降不能保证找到全局最小值，如左图；线性回归中线性函数是线性的，代价函数是凸函数（只有全局最小值），梯度下降一定能找到全局最小值（若学习率合适的话），如右图。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220160129545.png" alt="image-20241220160129545" style="zoom:50%;" /><p>我们换成交叉熵损失，当y&#x3D;1时，由于$h_\theta(x)$的取值范围在0~1，$cost(h_\theta(x),y)$的图像如下.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220161624207.png" alt="image-20241220161624207" style="zoom:50%;" /><p>当y&#x3D;0时，由于$h_\theta(x)$的取值范围在0~1，$cost(h_\theta(x),y)$的图像如下.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220162203207.png" alt="image-20241220162203207" style="zoom:50%;" /><p>使用交叉熵损失后，总体上<strong>代价函数$J(\theta)$是凸函数，只有一个全局最小值，可以使用梯度下降进行参数的拟合。</strong></p><p>因为y的取值只有0和1，将以上的两个式子可以简化成下面的一个式子，即单个样本的损失函数$cost(h_\theta(x),y)&#x3D;-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220211309704.png" alt="image-20241220211309704" style="zoom:50%;" /><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>对$J(\theta)$使用梯度下降算法，计算$J(\theta)$对$\theta_j$的偏导（下图中的绿框）。得到参数$\theta_j$的更新公式如下.可以用矢量化的方法进行参数的同步更新。</p><p>发现<strong>logistic回归和线性回归参数更新的公式是相同的</strong>。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220212149594.png" alt="image-20241220212149594" style="zoom:50%;" /><h2 id="高级优化算法"><a href="#高级优化算法" class="headerlink" title="高级优化算法"></a>高级优化算法</h2><p>除梯度下降外，还有conjugate gradient（共轭梯度法），BFGS，L-BFGS等。这些高级算法的优点是：（1）不需要手动选择学习率$\alpha$（2）比梯度下降更快。缺点：更复杂</p><p>优化算法的步骤如下：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220230319320.png" alt="image-20241220230319320" style="zoom:50%;" /><h2 id="多元分类：一对多"><a href="#多元分类：一对多" class="headerlink" title="多元分类：一对多"></a>多元分类：一对多</h2><p>当我们要进行3分类时，我们可以分别训练三个logistic回归模型。我们制作伪数据集，将第一类标记为正类，其余两类标记为负类，训练得到分类器1为$h_\theta^{(1)}(x)&#x3D;P(y&#x3D;1|x:\theta)$即当特征向量为x时，为第一类的可能性；同理训练$h_\theta^{(2)}(x)$和$h_\theta^{(3)}(x)$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220231128520.png" alt="image-20241220231128520" style="zoom:50%;" /><p>总结：对于n分类问题，我们训练n个分类器。第i个分类器$h_\theta^{(i)}(x)$表示$y&#x3D;i$的可能性。当要预测新的输入$x$时，将$x$输入n个分类器，全部进行判别，选输出最大那个，表示预测的类别。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241220231831610.png" alt="image-20241220231831610" style="zoom:50%;" />]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.多元线性回归</title>
    <link href="/2025/02/16/4%20%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <url>/2025/02/16/4%20%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h1 id="4-多元线性回归"><a href="#4-多元线性回归" class="headerlink" title="4 多元线性回归"></a>4 多元线性回归</h1><h2 id="多特征"><a href="#多特征" class="headerlink" title="多特征"></a>多特征</h2><p>设有n个特征，$x^{(i)}$表示第i个样本的特征向量，形状为$n\times 1$。$x^{(i)}_j$表示第i个样本的第j个特征的值。</p><p>特征有多个时，我们可以假设$h_{\theta}(x)&#x3D;\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$,可以让$x_0&#x3D;1$，这样$h_\theta(x)&#x3D;\theta^{T}x$</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241214161929934.png" alt="image-20241214161929934" style="zoom:50%;" /><p>梯度下降更新参数，左图为只有1个特征，右图为有多个特征时：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241214162959309.png" alt="image-20241214162959309" style="zoom:50%;" /><p>如何让梯度下降收敛更快：（1）特征缩放Feature Scaling（2）学习率</p><h2 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h2><p>当特征值的<strong>取值范围</strong>差距很大时，会相应的导致梯度差距很大，参数更新可能左右横跳，导致收敛速度变慢，如左图；</p><p>我们可以让变量除以极差（max-min）来进行特征缩放，让变量取值范围相近，缓解这一问题</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241214164557345.png" alt="image-20241214164557345" style="zoom:50%;" /><p>左右横跳是因为θ1的偏导相对θ2太大，相对的θ1下降步数太大，从山谷这头一下子迈到山谷那头，永远没头。</p><p><strong>均值归一化（Mean Normalization）</strong>：将数据的每个特征调整为均值为 0，范围标准化为固定区间（通常是 [-1, 1]）</p><p>$x_{norm}&#x3D;\frac{x-u}{x_{max}-x_{min}}$，其中$u$为均值</p><p><strong>标准化（Standardization）</strong>：调整为均值为 0，标准差为 1，适合于分布接近正态分布的数据</p><p>$x_{stand}&#x3D;\frac{x-u}{\sigma}$, 其中$\sigma$为标准差</p><h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><p>如何确保梯度下降正常工作？（1）绘制损失函数随迭代次数的<strong>曲线图</strong>。损失函数逐步下降说明正常工作；当损失函数较为平坦时，说明已经收敛（2）自动收敛测试，当某次迭代后，学习率下降很少$&lt;\epsilon$说明已经收敛, 但$\epsilon$取值较难设定。常用方法一</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241216163630788.png" alt="image-20241216163630788" style="zoom:50%;" /><p>当损失函数曲线逐步上升或成屁股形时，说明学习率取值过大，应调小学习率</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241216164203898.png" alt="image-20241216164203898" style="zoom:50%;" /><p><strong>当学习率充分小时，损失函数的值会逐步下降。但梯度下降算法会收敛的很慢。</strong></p><p>总结：学习率太小会收敛很慢；学习率太大，损失会增加；可能不收敛；可能收敛很慢。</p><p>如何选择学习率：先确定学习率的最小最大值，再在区间内尝试找到可以快速收敛的学习率。可以每3倍3倍的尝试。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241216164544635.png" alt="image-20241216164544635" style="zoom:50%;" /><h2 id="特征和多项式回归"><a href="#特征和多项式回归" class="headerlink" title="特征和多项式回归"></a>特征和多项式回归</h2><p>设计特征：我们可以不使用$h_{\theta}(x)&#x3D;\theta_0+\theta_1\times x_1+\theta_2\times x_2$，其中$x_1$为房子长度，$x_2$为房子宽度。我们可以设计特征房子面积$x&#x3D;x_1\times x_2$，令$h_{\theta}(x)&#x3D;\theta_0+\theta_1\times x$</p><p>多项式回归：分析数据点的分布情况，我们可以使用某个特征的根号项、二次项、三次项来作为函数的类型来进行拟合。注意：在有多次项的时候，特征缩放显得更加重要。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241216170757054.png" alt="image-20241216170757054" style="zoom:50%;" /><h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>求解参数$\theta$的方法：（1）梯度下降、正规方程</p><p>思想：令损失函数的偏导为0，解出$\theta$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241216191324398.png" alt="image-20241216191324398" style="zoom:50%;" /><p>方法：在数据中加入$x_0$特征这一列并初始化为1，将数据的每一项特征作为矩阵$X$的一列，所有数据的标签作为向量$y$。则$\theta&#x3D;(X^TX)^{-1}X^Ty$.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241216191436816.png" alt="image-20241216191436816" style="zoom:50%;" /><p>梯度下降vs正规方程：（1）梯度下降需要选择学习率，并且需要迭代多次；而正规方程不需学习率，不需迭代。（2）梯度下降当n很大时仍有效，而正规方程时间复杂度较高。</p><p>计算逆矩阵的复杂度为$O(n^3)$，当$n\le1e4$时使用正规方程，反之使用梯度下降。而且梯度下降在以后复杂方法中也可以求解，正规方程可能不可。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241216191836703.png" alt="image-20241216191836703" style="zoom:50%;" /><p>当（1）含有多余特征时，其中某些特征存在线性相关关系（2）特征太多时，样本数&lt;&#x3D;特征数。会出现矩阵$X^TX$不可逆的情况。尝试（1）删除多余的特征（2）删除某些特征或正则化</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241216200344136.png" alt="image-20241216200344136" style="zoom:50%;" /><h2 id="矢量化"><a href="#矢量化" class="headerlink" title="矢量化"></a>矢量化</h2><p>$h_\theta(x)&#x3D;\sum_{j&#x3D;0}^{n}\theta_jx_j$对应未矢量化的代码(for)，左半部分。</p><p>$h_\theta(x)&#x3D;\theta^Tx$对应矢量化的代码，右半部分。运行效率更快。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241219165803645.png" alt="image-20241219165803645" style="zoom:50%;" /><p>在梯度下降更新参数$\theta$时，我们可以使用矢量化完成<strong>参数同步更新</strong>、<strong>缩短代码</strong>、<strong>更高效</strong>。</p><p>我们可以将参数更新的公式记为$\theta:&#x3D;\theta-\alpha\delta$.这里$\theta\in\mathbb{R}^{n+1},\alpha\in\mathbb{R},\delta\in\mathbb{R}^{n+1}.$ where $\delta&#x3D;\frac{1}{m}\sum^{m}<em>{i&#x3D;1}{(h</em>{\theta}(x^{(i)})-y^{(i)})(x^{i})}$.</p><p>下面说明为什么$\delta$是n+1维的向量。</p><p>求和号中的$(h_{\theta}(x^{(i)})-y^{(i)})\in\mathbb{R}$表示用第i个样本的预测值-实际值，是一个数。$(x^{i})\in\mathbb{R}^{n+1}$表示第i个样本的特征向量，是一个向量。得证。</p><p>将求和号展开，即$\sum^{m}<em>{i&#x3D;1}{(h</em>{\theta}(x^{(i)})-y^{(i)})(x^{i})}&#x3D;(h_{\theta}(x^{(1)})-y^{(1)})x^{(1)}+(h_{\theta}(x^{(2)})-y^{(2)})x^{(2)}+…$，其中$(h_{\theta}(x^{(1)})-y^{(1)}),(h_{\theta}(x^{(2)})-y^{(2)})$是数，$x^{(1)},x^{(2)}$是向量。</p><p>其中将$x^{(i)}$展开为向量，只关注$x^{(i)}_0$的项，相乘相加即为$\delta_0$。</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241219165707439.png" alt="image-20241219165707439" style="zoom:50%;" />]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.线代知识</title>
    <link href="/2025/02/16/3%20%E7%BA%BF%E4%BB%A3%E7%9F%A5%E8%AF%86/"/>
    <url>/2025/02/16/3%20%E7%BA%BF%E4%BB%A3%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[<h1 id="3-线代知识"><a href="#3-线代知识" class="headerlink" title="3 线代知识"></a>3 线代知识</h1><p>矩阵：常用大写字母表示$A, B, X\in \mathbb{R}^{2\times3}$</p><p>向量：维度为$n\times1$的矩阵。常用小写字母表示$a, b, x\in \mathbb{R}^{3}$</p><p>矩阵1为数据矩阵，矩阵2为参数矩阵（第i列表示第i个函数的参数），答案矩阵（第i列是用第i个函数得到的预测结果）</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241214144519665.png" alt="image-20241214144519665" style="zoom:50%;" /><p>无交换律：$A\times B \ne B\times A$</p><p>有结合律：$A\times (B\times C)&#x3D;(A\times B)\times C$</p><p>单位矩阵$I$，设$A\in \mathbb{R}^{m\times n}$, 有$A\times I_{n\times n}&#x3D;I_{m\times m}\times A&#x3D;A$</p><p>逆矩阵，只有方阵才可能存在逆矩阵，满足：$AA^{-1}&#x3D;A^{-1}A&#x3D;I$.没有逆矩阵的成为奇异矩阵或退化矩阵</p><p>转置：设$B&#x3D;A^{T}, 满足B_{ij}&#x3D;A_{ji}$</p>]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.单变量线性回归</title>
    <link href="/2025/02/16/2%20%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <url>/2025/02/16/2%20%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h1 id="2-单变量线性回归"><a href="#2-单变量线性回归" class="headerlink" title="2 单变量线性回归"></a>2 单变量线性回归</h1><h2 id="模型描述"><a href="#模型描述" class="headerlink" title="模型描述"></a>模型描述</h2><p>是监督学习，回归任务。</p><p>让模型在数据集中学到一个函数$h(x)&#x3D;\theta_0+\theta_1x$，完成从x到y的映射</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>我们的问题是如何找到$ \theta_0和\theta_1 $ 让预测更加准确。</p><p>我们定义损失函数如下，均方误差（回归任务常用）：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241213201825642.png" alt="image-20241213201825642" style="zoom: 50%;" /><p>任务转化为找到$ \theta_0和\theta_1 $ 让损失函数最小。</p><p>我们假设$h(x)&#x3D;\theta_1x$ ，$J(\theta_1)&#x3D;\frac{1}{2m}\sum_{i&#x3D;1}^{m}{(h(x^{(i)})-y^{(i)})^2}$ </p><p>每个$\theta_1$对应一条直线$h(x)$, 对应一个数值$J(\theta_1)$，这样可以画出右图</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241213203136416.png" alt="image-20241213203136416" style="zoom:50%;" /><p>我们假设$h(x)&#x3D;\theta_0+\theta_1x$ ，$J(\theta_1)&#x3D;\frac{1}{2m}\sum_{i&#x3D;1}^{m}{(h(x^{(i)})-y^{(i)})^2}$ </p><p>每对$(\theta_0,\theta_1)$对应一条直线$h(x)$, 对应一个数值$J(\theta_0,\theta_1)$，这样可以画出三维的碗状图，这里我们使用了等高线图</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241213205011397.png" alt="image-20241213205011397" style="zoom:50%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241213204924666.png" alt="image-20241213204924666" style="zoom:50%;" /><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>用来最小化任意损失函数$J(\theta_0,,,\theta_n)$</p><p>算法步骤：（1）初始化$\theta_0,,,\theta_n$的值（2）改变$\theta_0,,,\theta_n$, 来降低$J$, 重复该步直至到达全局最优或局部最优</p><p>问题：不同的初始化可能会进入不同的局部最优</p><p>参数的更新公式如下，其中$\alpha$是学习率，表示迈步的大小，偏导表示迈步的方向</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241213211027518.png" alt="image-20241213211027518" style="zoom: 50%;" /><p>注意参数需要<strong>同时</strong>更新：</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241213211835351.png" alt="image-20241213211835351" style="zoom:50%;" /><p>偏导的意义：指导参数更新的方向<img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241213212549849.png" alt="image-20241213212549849" style="zoom:50%;" /></p><p>学习率的意义：当学习率太小，梯度下降会很慢，需要很多步才能到达最优值。学习率太大，可能无法收敛甚至发散</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241213213608408.png" alt="image-20241213213608408" style="zoom:50%;" /><p><strong>即使学习率不变，梯度下降方法也能找到局部最小值</strong>。当靠近局部最小值时，梯度会变小，更新步幅也相应变小；当到达局部最小值时，梯度为0，更新步幅为0.</p><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241213213911710.png" alt="image-20241213213911710" style="zoom:50%;" /><img src="C:\Users\Dongxuan\AppData\Roaming\Typora\typora-user-images\image-20241214004642966.png" alt="image-20241214004642966" style="zoom:50%;" />]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.机器学习绪论</title>
    <link href="/2025/02/16/1%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%AA%E8%AE%BA/"/>
    <url>/2025/02/16/1%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%AA%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="1-机器学习绪论"><a href="#1-机器学习绪论" class="headerlink" title="1 机器学习绪论"></a>1 机器学习绪论</h1><h2 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h2><p>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. </p><p>机器学习主要分为监督学习（supervised learning）、无监督学习（unsupervised learning）、其他（强化学习、推荐系统）</p><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>监督学习：给出正确标签</p><p>若欲预测的是离散值，为分类Classification。若欲预测的是连续值，为回归Regression。</p><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p>无监督学习：从没有标记的数据集中，找出数据中的结构。eg：聚类</p>]]></content>
    
    
    <categories>
      
      <category>吴恩达机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
